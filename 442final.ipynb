{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnDBPA4C1PiH",
        "outputId": "ee923fdb-dd34-4b94-847c-b416f3cee4a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '442final'"
      ],
      "metadata": {
        "id": "UBMGrK372Nlx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n"
      ],
      "metadata": {
        "id": "b81MUQrP2PAL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(GOOGLE_DRIVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GPSB9fk2RRt",
        "outputId": "94aa3238-c7c2-427b-8816-1ca44e062737"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/442final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "UEFs83s32SxE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#navigate to folder\n",
        "%cd /content/drive/MyDrive/442final\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0APgro8r2U8e",
        "outputId": "4e1e6456-50d9-48e7-9770-99d3a777d319"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/442final\n",
            "Sun Apr 28 21:50:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   34C    P8              11W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip data and then step back out\n",
        "%cd data\n",
        "!unzip datazip\n",
        "!ls\n",
        "%cd .."
      ],
      "metadata": {
        "id": "nn484LOD2ZCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/archive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TLvcBDg5ryX",
        "outputId": "c4d9b405-bc8e-4144-adb5-7e7e6711bb78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0    12   16   2    23   27   30   34\t 38   41   45   49   52   56   6    8\t\t +_sampled\n",
            " 1    13   17   20   24   28   31   35\t 39   42   46   5    53   57   60   9\t\t -_sampled\n",
            " 10   14   18   21   25   29   32   36\t 4    43   47   50   54   58   61  '(_sampled'\t x_sampled\n",
            " 11   15   19   22   26   3    33   37\t 40   44   48   51   55   59   7   ')_sampled'\t Ў_sampled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "##make renamed directory, code from Yuvraj Joshi\n",
        "##I will be using this modified dataset to train my models\n",
        "\n",
        "def rename_and_copy_folders(root_dir, dest_dir):\n",
        "    # Define mapping for renaming specific items\n",
        "    mapping = {\n",
        "        '+_sampled': '+',\n",
        "        '-_sampled': '-',\n",
        "        '(_sampled': '(',\n",
        "        ')_sampled': ')',\n",
        "        'x_sampled': '*',\n",
        "        'Ў_sampled': '÷',\n",
        "        \"10\": \"A\", \"11\": \"B\", \"12\": \"C\", \"13\": \"D\", \"14\": \"E\", \"15\": \"F\",\n",
        "        \"16\": \"G\", \"17\": \"H\", \"18\": \"I\", \"19\": \"J\", \"20\": \"K\", \"21\": \"L\",\n",
        "        \"22\": \"M\", \"23\": \"N\", \"24\": \"O\", \"25\": \"P\", \"26\": \"Q\", \"27\": \"R\",\n",
        "        \"28\": \"S\", \"29\": \"T\", \"30\": \"U\", \"31\": \"V\", \"32\": \"W\", \"33\": \"X\",\n",
        "        \"34\": \"Y\", \"35\": \"Z\", \"36\": \"a\", \"37\": \"b\", \"38\": \"c\", \"39\": \"d\",\n",
        "        \"40\": \"e\", \"41\": \"f\", \"42\": \"g\", \"43\": \"h\", \"44\": \"i\", \"45\": \"j\",\n",
        "        \"46\": \"k\", \"47\": \"l\", \"48\": \"m\", \"49\": \"n\", \"50\": \"o\", \"51\": \"p\",\n",
        "        \"52\": \"q\", \"53\": \"r\", \"54\": \"s\", \"55\": \"t\", \"56\": \"u\", \"57\": \"v\",\n",
        "        \"58\": \"w\", \"59\": \"x\", \"60\": \"y\", \"61\": \"z\",\n",
        "    }\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "        # Get the name of the current directory\n",
        "        dir_name = os.path.basename(subdir)\n",
        "        # Get the new name from mapping\n",
        "        new_name = mapping.get(dir_name, dir_name)\n",
        "        # Construct the new directory path\n",
        "        new_dir_path = os.path.join(dest_dir, new_name)\n",
        "        # Create the new directory\n",
        "        os.makedirs(new_dir_path, exist_ok=True)\n",
        "        # Copy files to the new directory, taking only every 10th file\n",
        "        for i, file in enumerate(files):\n",
        "            if i % 10 == 0:  # This checks if the file index is a multiple of 10\n",
        "                src_file_path = os.path.join(subdir, file)\n",
        "                print(\"source file path: \", src_file_path)\n",
        "\n",
        "                dest_file_path = os.path.join(new_dir_path, file)\n",
        "                print(\"dest file path: \", dest_file_path)\n",
        "                shutil.copyfile(src_file_path, dest_file_path)\n",
        "\n",
        "# Specify data paths\n",
        "root_directory = \"/content/drive/MyDrive/442final/data/archive\"\n",
        "destination_directory = \"/content/drive/MyDrive/442final/renamed\"\n",
        "print(\"call\")\n",
        "rename_and_copy_folders(root_directory, destination_directory)\n"
      ],
      "metadata": {
        "id": "RcSYX71P2kMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls renamed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKsm56xn2vsx",
        "outputId": "5c2819cb-7180-4ce8-9540-bb3149c5a3bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'('   +   0   3   6   9   b   C   e   F   h   I   k   L   n   O   q   R   t   U   w   X   z\n",
            "')'   -   1   4   7   a   B   d   E   g   H   j   K   m   N   p   Q   s   T   v   W   y   Z\n",
            "'*'   ÷   2   5   8   A   c   D   f   G   i   J   l   M   o   P   r   S   u   V   x   Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr-zc8inVe1t",
        "outputId": "f0ed2f65-d1fd-47be-a0d5-2596edc1b25d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442final.ipynb\t\t\t\t\t\t    mobilenetv3_symbol_classifier_fold_5.h5\n",
            "data\t\t\t\t\t\t\t    mobilenetv3_symbol_classifier_tuned.h5\n",
            "mobilenetv3_symbol_classifier_2.h5\t\t\t    mobilenetv3_symbol_cv_tuned.h5\n",
            "mobilenetv3_symbol_classifier_3.h5\t\t\t    my_dir\n",
            "mobilenetv3_symbol_classifier_5.h5\t\t\t    renamed\n",
            "mobilenetv3_symbol_classifier_6_res_dropout_every_layer.h5  renamed_data.zip\n",
            "mobilenetv3_symbol_classifier_6_res.h5\t\t\t    test_set\n",
            "mobilenetv3_symbol_classifier_7.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##imports for ML\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n"
      ],
      "metadata": {
        "id": "NfivJp5B99gJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##My first morel, M2a\n",
        "\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64  # Updated to match the input shape of the model\n",
        "input_shape = (img_height, img_width, 1)  # Assuming grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')  # Ensure grayscale mode if images are grayscale\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')  # Ensure grayscale mode if images are grayscale\n",
        "\n",
        "num_classes = 68  # 10 digits + 4 symbols (+, -, ×, ÷) + 2 parentheses ( )\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "initial_learning_rate = 0.001  # Starting with a larger learning rate\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_2.h5\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EhE8p5KU-HqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict class for a single image\n",
        "def predict_class(image_path, model):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width), color_mode='grayscale')\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(prediction)\n",
        "    class_labels = sorted(train_generator.class_indices.keys(), key=lambda x: train_generator.class_indices[x])\n",
        "    predicted_class = class_labels[predicted_class_index]\n",
        "    return predicted_class\n",
        "\n",
        "# Example usage: Predicting class for a single image\n",
        "image_path = \"/content/drive/MyDrive/442final/renamed/0/3637.png\"  # Adjust with your image path\n",
        "predicted_class = predict_class(image_path, model)\n",
        "print(\"Predicted class:\", predicted_class)"
      ],
      "metadata": {
        "id": "X75Q_ImVwPK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##make a test set to compare all of my models\n",
        "##saved in directory test_set\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def create_test_set(root_dir, dest_dir):\n",
        "    # Define mapping for renaming specific items\n",
        "    mapping = {\n",
        "        '+_sampled': '+',\n",
        "        '-_sampled': '-',\n",
        "        '(_sampled': '(',\n",
        "        ')_sampled': ')',\n",
        "        'x_sampled': '*',\n",
        "        'Ў_sampled': '÷',\n",
        "        \"10\": \"A\", \"11\": \"B\", \"12\": \"C\", \"13\": \"D\", \"14\": \"E\", \"15\": \"F\",\n",
        "        \"16\": \"G\", \"17\": \"H\", \"18\": \"I\", \"19\": \"J\", \"20\": \"K\", \"21\": \"L\",\n",
        "        \"22\": \"M\", \"23\": \"N\", \"24\": \"O\", \"25\": \"P\", \"26\": \"Q\", \"27\": \"R\",\n",
        "        \"28\": \"S\", \"29\": \"T\", \"30\": \"U\", \"31\": \"V\", \"32\": \"W\", \"33\": \"X\",\n",
        "        \"34\": \"Y\", \"35\": \"Z\", \"36\": \"a\", \"37\": \"b\", \"38\": \"c\", \"39\": \"d\",\n",
        "        \"40\": \"e\", \"41\": \"f\", \"42\": \"g\", \"43\": \"h\", \"44\": \"i\", \"45\": \"j\",\n",
        "        \"46\": \"k\", \"47\": \"l\", \"48\": \"m\", \"49\": \"n\", \"50\": \"o\", \"51\": \"p\",\n",
        "        \"52\": \"q\", \"53\": \"r\", \"54\": \"s\", \"55\": \"t\", \"56\": \"u\", \"57\": \"v\",\n",
        "        \"58\": \"w\", \"59\": \"x\", \"60\": \"y\", \"61\": \"z\",\n",
        "    }\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "        # Get the name of the current directory\n",
        "        dir_name = os.path.basename(subdir)\n",
        "        # Get the new name from mapping\n",
        "        new_name = mapping.get(dir_name, dir_name)\n",
        "        # Construct the new directory path\n",
        "        new_dir_path = os.path.join(dest_dir, new_name)\n",
        "        # Create the new directory\n",
        "        os.makedirs(new_dir_path, exist_ok=True)\n",
        "        # Copy files to the new directory, taking only every 10th file\n",
        "        for i, file in enumerate(files):\n",
        "           if i % 21 == 0 and i % 10 != 0:\n",
        "                src_file_path = os.path.join(subdir, file)\n",
        "                print(\"source file path: \", src_file_path)\n",
        "\n",
        "                dest_file_path = os.path.join(new_dir_path, file)\n",
        "                print(\"dest file path: \", dest_file_path)\n",
        "                shutil.copyfile(src_file_path, dest_file_path)\n",
        "\n",
        "# Specify data paths\n",
        "root_directory = \"/content/drive/MyDrive/442final/data/archive\"\n",
        "destination_directory = \"/content/drive/MyDrive/442final/test_set\"\n",
        "print(\"call\")\n",
        "create_test_set(root_directory, destination_directory)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qa1dH5YsDx2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_2.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yo__PipHKzc",
        "outputId": "4fea2b22-3deb-41e0-9263-5a1d6cbe1c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 32, 32, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 16, 16, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 8, 8, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               2097408   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 68)                17476     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2207556 (8.42 MB)\n",
            "Trainable params: 2207556 (8.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 6s 60ms/step - loss: 1.1317 - accuracy: 0.6014\n",
            "Test Loss: 1.131656527519226\n",
            "Test Accuracy: 0.6014093160629272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model architecture from Yuvrav Joshi\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Reshape\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming you have already imported the necessary modules\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64\n",
        "input_shape = (img_height, img_width, 1)  # Grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "# Model configuration\n",
        "num_classes = 68\n",
        "\n",
        "# Build the CNN model inspired by modelB but adapted for your dataset\n",
        "model = Sequential([\n",
        "    Conv2D(32, (5, 5), activation='relu', padding='same', input_shape=input_shape, kernel_constraint=MaxNorm(3)),\n",
        "    Conv2D(32, (5, 5), activation='relu', kernel_constraint=MaxNorm(3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(1024, activation='relu', kernel_constraint=MaxNorm(3)),\n",
        "    Dense(512, activation='relu', kernel_constraint=MaxNorm(3)),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_3.h5\")\n",
        "\n",
        "# Function to predict class for a single image\n",
        "def predict_class(image_path, model):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width), color_mode='grayscale')\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(prediction)\n",
        "    class_labels = sorted(train_generator.class_indices.keys(), key=lambda x: train_generator.class_indices[x])\n",
        "    predicted_class = class_labels[predicted_class_index]\n",
        "    return predicted_class\n",
        "\n",
        "# Example usage: Predicting class for a single image\n",
        "image_path = \"/content/drive/MyDrive/442final/renamed/0/3637.png\"  # Adjust with your image path\n",
        "predicted_class = predict_class(image_path, model)\n",
        "print(\"Predicted class:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPVM2_NDH1TO",
        "outputId": "98491238-c504-4b01-deae-73b2025a65e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6120 images belonging to 68 classes.\n",
            "Found 1496 images belonging to 68 classes.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 64, 64, 32)        832       \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 60, 60, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 30, 30, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 28800)             0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1024)              29492224  \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 68)                34884     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30078372 (114.74 MB)\n",
            "Trainable params: 30078372 (114.74 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "383/383 [==============================] - 22s 53ms/step - loss: 3.9288 - accuracy: 0.0458 - val_loss: 3.5183 - val_accuracy: 0.0675\n",
            "Epoch 2/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.2988 - accuracy: 0.1051 - val_loss: 2.9435 - val_accuracy: 0.1604\n",
            "Epoch 3/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.8849 - accuracy: 0.1592 - val_loss: 2.5829 - val_accuracy: 0.2226\n",
            "Epoch 4/50\n",
            "383/383 [==============================] - 21s 54ms/step - loss: 2.6162 - accuracy: 0.2118 - val_loss: 2.3565 - val_accuracy: 0.2667\n",
            "Epoch 5/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.4695 - accuracy: 0.2343 - val_loss: 2.2301 - val_accuracy: 0.2934\n",
            "Epoch 6/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 2.3487 - accuracy: 0.2598 - val_loss: 2.1253 - val_accuracy: 0.3095\n",
            "Epoch 7/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.2280 - accuracy: 0.2863 - val_loss: 2.0445 - val_accuracy: 0.3175\n",
            "Epoch 8/50\n",
            "383/383 [==============================] - 19s 51ms/step - loss: 2.1651 - accuracy: 0.3007 - val_loss: 2.0005 - val_accuracy: 0.3516\n",
            "Epoch 9/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 2.0771 - accuracy: 0.3327 - val_loss: 1.9055 - val_accuracy: 0.3683\n",
            "Epoch 10/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.0527 - accuracy: 0.3389 - val_loss: 1.8676 - val_accuracy: 0.3790\n",
            "Epoch 11/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.9908 - accuracy: 0.3526 - val_loss: 1.8081 - val_accuracy: 0.4051\n",
            "Epoch 12/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.9461 - accuracy: 0.3557 - val_loss: 1.7727 - val_accuracy: 0.4078\n",
            "Epoch 13/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.8987 - accuracy: 0.3704 - val_loss: 1.7800 - val_accuracy: 0.4091\n",
            "Epoch 14/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.8749 - accuracy: 0.3819 - val_loss: 1.7365 - val_accuracy: 0.4011\n",
            "Epoch 15/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.8610 - accuracy: 0.3809 - val_loss: 1.8011 - val_accuracy: 0.4017\n",
            "Epoch 16/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.8197 - accuracy: 0.3935 - val_loss: 1.6896 - val_accuracy: 0.4318\n",
            "Epoch 17/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.7939 - accuracy: 0.4021 - val_loss: 1.7719 - val_accuracy: 0.4151\n",
            "Epoch 18/50\n",
            "383/383 [==============================] - 19s 51ms/step - loss: 1.7941 - accuracy: 0.3958 - val_loss: 1.6670 - val_accuracy: 0.4298\n",
            "Epoch 19/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.7766 - accuracy: 0.4060 - val_loss: 1.7117 - val_accuracy: 0.4325\n",
            "Epoch 20/50\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 1.7415 - accuracy: 0.4057 - val_loss: 1.7345 - val_accuracy: 0.4211\n",
            "Epoch 21/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.7069 - accuracy: 0.4297 - val_loss: 1.5684 - val_accuracy: 0.4626\n",
            "Epoch 22/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.7088 - accuracy: 0.4351 - val_loss: 1.6491 - val_accuracy: 0.4372\n",
            "Epoch 23/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.6993 - accuracy: 0.4255 - val_loss: 1.5714 - val_accuracy: 0.4612\n",
            "Epoch 24/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.6890 - accuracy: 0.4294 - val_loss: 1.5462 - val_accuracy: 0.4759\n",
            "Epoch 25/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.6861 - accuracy: 0.4299 - val_loss: 1.5147 - val_accuracy: 0.4626\n",
            "Epoch 26/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 1.6562 - accuracy: 0.4487 - val_loss: 1.5560 - val_accuracy: 0.4733\n",
            "Epoch 27/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.6551 - accuracy: 0.4469 - val_loss: 1.5358 - val_accuracy: 0.4759\n",
            "Epoch 28/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.6414 - accuracy: 0.4549 - val_loss: 1.4921 - val_accuracy: 0.4960\n",
            "Epoch 29/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.6186 - accuracy: 0.4531 - val_loss: 1.5352 - val_accuracy: 0.4773\n",
            "Epoch 30/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.6023 - accuracy: 0.4632 - val_loss: 1.5376 - val_accuracy: 0.4840\n",
            "Epoch 31/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5986 - accuracy: 0.4637 - val_loss: 1.4936 - val_accuracy: 0.5160\n",
            "Epoch 32/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.5732 - accuracy: 0.4745 - val_loss: 1.4983 - val_accuracy: 0.4866\n",
            "Epoch 33/50\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 1.6255 - accuracy: 0.4505 - val_loss: 1.5358 - val_accuracy: 0.4953\n",
            "Epoch 34/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5964 - accuracy: 0.4565 - val_loss: 1.6521 - val_accuracy: 0.4619\n",
            "Epoch 35/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5773 - accuracy: 0.4618 - val_loss: 1.5018 - val_accuracy: 0.5060\n",
            "Epoch 36/50\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 1.5719 - accuracy: 0.4616 - val_loss: 1.5467 - val_accuracy: 0.4860\n",
            "Epoch 37/50\n",
            "383/383 [==============================] - 19s 51ms/step - loss: 1.5583 - accuracy: 0.4735 - val_loss: 1.5344 - val_accuracy: 0.4886\n",
            "Epoch 38/50\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 1.5714 - accuracy: 0.4766 - val_loss: 1.5221 - val_accuracy: 0.4786\n",
            "Epoch 39/50\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 1.5587 - accuracy: 0.4706 - val_loss: 1.4821 - val_accuracy: 0.4953\n",
            "Epoch 40/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 1.5432 - accuracy: 0.4809 - val_loss: 1.4342 - val_accuracy: 0.5107\n",
            "Epoch 41/50\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 1.5701 - accuracy: 0.4688 - val_loss: 1.5263 - val_accuracy: 0.4820\n",
            "Epoch 42/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5512 - accuracy: 0.4761 - val_loss: 1.4606 - val_accuracy: 0.5087\n",
            "Epoch 43/50\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 1.5363 - accuracy: 0.4719 - val_loss: 1.4463 - val_accuracy: 0.5167\n",
            "Epoch 44/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5523 - accuracy: 0.4763 - val_loss: 1.4579 - val_accuracy: 0.5134\n",
            "Epoch 45/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5508 - accuracy: 0.4714 - val_loss: 1.4786 - val_accuracy: 0.4940\n",
            "Epoch 46/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5493 - accuracy: 0.4748 - val_loss: 1.5059 - val_accuracy: 0.4866\n",
            "Epoch 47/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5300 - accuracy: 0.4819 - val_loss: 1.4588 - val_accuracy: 0.4926\n",
            "Epoch 48/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5318 - accuracy: 0.4822 - val_loss: 1.5272 - val_accuracy: 0.4926\n",
            "Epoch 49/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 1.5151 - accuracy: 0.4812 - val_loss: 1.5319 - val_accuracy: 0.5007\n",
            "Epoch 50/50\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 1.4912 - accuracy: 0.4941 - val_loss: 1.3885 - val_accuracy: 0.5207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 115ms/step\n",
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test Joshi's CNN\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_3.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'categorical' because your model uses categorical crossentropy\n",
        "    color_mode='grayscale',  # Match the color mode to how the model was trained\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytmzCwtJMMBP",
        "outputId": "4a9bedc6-4819-4dd7-c972-b46f85b55fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 64, 64, 32)        832       \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 60, 60, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 30, 30, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 28800)             0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1024)              29492224  \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 68)                34884     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30078372 (114.74 MB)\n",
            "Trainable params: 30078372 (114.74 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 7s 63ms/step - loss: 1.2439 - accuracy: 0.5527\n",
            "Test Loss: 1.2439252138137817\n",
            "Test Accuracy: 0.5526960492134094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#M2a with L2 regularization and increased dropout rate.\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64\n",
        "input_shape = (img_height, img_width, 1)  # Assuming grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "num_classes = 68\n",
        "\n",
        "# Build the CNN model with increased dropout and L2 regularization\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),  # Increased dropout rate\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=30,  #last time pleatued around 20,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_4.h5\")\n",
        "\n",
        "##ended this process early because it showed low accuracy and a slow growth trend\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nnuQttKCMWst",
        "outputId": "2b88d040-9add-420f-eacf-1787bcf181c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6120 images belonging to 68 classes.\n",
            "Found 1496 images belonging to 68 classes.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 64, 64, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 32, 32, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPoolin  (None, 16, 16, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 8, 8, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               2097408   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 68)                17476     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2207556 (8.42 MB)\n",
            "Trainable params: 2207556 (8.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "383/383 [==============================] - 22s 53ms/step - loss: 4.3718 - accuracy: 0.0307 - val_loss: 3.9594 - val_accuracy: 0.0461\n",
            "Epoch 2/30\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.9612 - accuracy: 0.0376 - val_loss: 3.8182 - val_accuracy: 0.0608\n",
            "Epoch 3/30\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 3.8647 - accuracy: 0.0508 - val_loss: 3.7200 - val_accuracy: 0.0662\n",
            "Epoch 4/30\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.7792 - accuracy: 0.0557 - val_loss: 3.6718 - val_accuracy: 0.0782\n",
            "Epoch 5/30\n",
            "383/383 [==============================] - 20s 53ms/step - loss: 3.7523 - accuracy: 0.0644 - val_loss: 3.6177 - val_accuracy: 0.0816\n",
            "Epoch 6/30\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.7147 - accuracy: 0.0673 - val_loss: 3.5886 - val_accuracy: 0.0969\n",
            "Epoch 7/30\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.6291 - accuracy: 0.0791 - val_loss: 3.4772 - val_accuracy: 0.1150\n",
            "Epoch 8/30\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 3.5963 - accuracy: 0.0845 - val_loss: 3.4619 - val_accuracy: 0.1156\n",
            "Epoch 9/30\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.5583 - accuracy: 0.0912 - val_loss: 3.3314 - val_accuracy: 0.1417\n",
            "Epoch 10/30\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 3.5078 - accuracy: 0.0966 - val_loss: 3.3030 - val_accuracy: 0.1437\n",
            "Epoch 11/30\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.4670 - accuracy: 0.0946 - val_loss: 3.2792 - val_accuracy: 0.1277\n",
            "Epoch 12/30\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.4291 - accuracy: 0.1031 - val_loss: 3.2193 - val_accuracy: 0.1424\n",
            "Epoch 13/30\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 3.4345 - accuracy: 0.0998 - val_loss: 3.2423 - val_accuracy: 0.1471\n",
            "Epoch 14/30\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.3899 - accuracy: 0.1082 - val_loss: 3.1611 - val_accuracy: 0.1524\n",
            "Epoch 15/30\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 3.3806 - accuracy: 0.1147 - val_loss: 3.1598 - val_accuracy: 0.1698\n",
            "Epoch 16/30\n",
            "383/383 [==============================] - 19s 51ms/step - loss: 3.3713 - accuracy: 0.1132 - val_loss: 3.1775 - val_accuracy: 0.1491\n",
            "Epoch 17/30\n",
            "266/383 [===================>..........] - ETA: 4s - loss: 3.3390 - accuracy: 0.1191"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-d8ee9976835b>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m#last time pleatued around 20,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#L2 regularization only\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64\n",
        "input_shape = (img_height, img_width, 1)  # Assuming grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "num_classes = 68\n",
        "\n",
        "# Build the CNN model with L2 regularization\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_4.h5\")\n",
        "\n",
        "##ended this process early because it showed low accuracy and a slow growth trend\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9bnmtqwqOgv3",
        "outputId": "b222c392-7269-4003-a7f9-137fd19ac277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6120 images belonging to 68 classes.\n",
            "Found 1496 images belonging to 68 classes.\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_13 (Conv2D)          (None, 64, 64, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 32, 32, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPooli  (None, 16, 16, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPooli  (None, 8, 8, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               2097408   \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 68)                17476     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2207556 (8.42 MB)\n",
            "Trainable params: 2207556 (8.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "383/383 [==============================] - 22s 53ms/step - loss: 4.2955 - accuracy: 0.0345 - val_loss: 3.9032 - val_accuracy: 0.0628\n",
            "Epoch 2/50\n",
            "383/383 [==============================] - 19s 51ms/step - loss: 3.8010 - accuracy: 0.0590 - val_loss: 3.6862 - val_accuracy: 0.0749\n",
            "Epoch 3/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 3.6908 - accuracy: 0.0693 - val_loss: 3.6284 - val_accuracy: 0.0809\n",
            "Epoch 4/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 3.5983 - accuracy: 0.0856 - val_loss: 3.5160 - val_accuracy: 0.0996\n",
            "Epoch 5/50\n",
            "383/383 [==============================] - 19s 51ms/step - loss: 3.4297 - accuracy: 0.1096 - val_loss: 3.3321 - val_accuracy: 0.1150\n",
            "Epoch 6/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 3.2453 - accuracy: 0.1397 - val_loss: 3.1884 - val_accuracy: 0.1684\n",
            "Epoch 7/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 3.1237 - accuracy: 0.1657 - val_loss: 3.0598 - val_accuracy: 0.1825\n",
            "Epoch 8/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 3.0336 - accuracy: 0.1850 - val_loss: 3.0430 - val_accuracy: 0.1805\n",
            "Epoch 9/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 2.9439 - accuracy: 0.2020 - val_loss: 2.9829 - val_accuracy: 0.2032\n",
            "Epoch 10/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 2.8812 - accuracy: 0.2203 - val_loss: 2.9279 - val_accuracy: 0.2172\n",
            "Epoch 11/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.8803 - accuracy: 0.2234 - val_loss: 2.8911 - val_accuracy: 0.2193\n",
            "Epoch 12/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 2.8192 - accuracy: 0.2340 - val_loss: 2.7930 - val_accuracy: 0.2393\n",
            "Epoch 13/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.7954 - accuracy: 0.2394 - val_loss: 2.8339 - val_accuracy: 0.2239\n",
            "Epoch 14/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 2.7676 - accuracy: 0.2453 - val_loss: 2.7880 - val_accuracy: 0.2406\n",
            "Epoch 15/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 2.7696 - accuracy: 0.2449 - val_loss: 2.8151 - val_accuracy: 0.2313\n",
            "Epoch 16/50\n",
            "383/383 [==============================] - 19s 51ms/step - loss: 2.7246 - accuracy: 0.2489 - val_loss: 2.7280 - val_accuracy: 0.2634\n",
            "Epoch 17/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.7330 - accuracy: 0.2528 - val_loss: 2.7975 - val_accuracy: 0.2487\n",
            "Epoch 18/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 2.7030 - accuracy: 0.2657 - val_loss: 2.7470 - val_accuracy: 0.2560\n",
            "Epoch 19/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.6957 - accuracy: 0.2665 - val_loss: 2.7217 - val_accuracy: 0.2520\n",
            "Epoch 20/50\n",
            "383/383 [==============================] - 20s 52ms/step - loss: 2.6723 - accuracy: 0.2644 - val_loss: 2.6403 - val_accuracy: 0.2841\n",
            "Epoch 21/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 2.6534 - accuracy: 0.2657 - val_loss: 2.6982 - val_accuracy: 0.2674\n",
            "Epoch 22/50\n",
            "383/383 [==============================] - 19s 51ms/step - loss: 2.6409 - accuracy: 0.2792 - val_loss: 2.5832 - val_accuracy: 0.2881\n",
            "Epoch 23/50\n",
            "383/383 [==============================] - 19s 50ms/step - loss: 2.6200 - accuracy: 0.2820 - val_loss: 2.6285 - val_accuracy: 0.2828\n",
            "Epoch 24/50\n",
            "383/383 [==============================] - 20s 51ms/step - loss: 2.6300 - accuracy: 0.2752 - val_loss: 2.6053 - val_accuracy: 0.2821\n",
            "Epoch 25/50\n",
            " 70/383 [====>.........................] - ETA: 13s - loss: 2.5789 - accuracy: 0.3107"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-de77f9597d0c>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#M2a with increased dropout rate\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64\n",
        "input_shape = (img_height, img_width, 1)  # Assuming grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "num_classes = 68\n",
        "\n",
        "# Build the CNN model with increased dropout rates\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.6),  # Increased dropout rate for higher model regularization\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_4.h5\")\n",
        "\n",
        "\n",
        "#colab is taking really long to rerun this and it got deleted\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "rM5LVo3rOjHq",
        "outputId": "889a3c01-4538-486c-a07b-acb806da9e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6120 images belonging to 68 classes.\n",
            "Found 1496 images belonging to 68 classes.\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_22 (Conv2D)          (None, 64, 64, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPooli  (None, 32, 32, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_21 (MaxPooli  (None, 16, 16, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_22 (MaxPooli  (None, 8, 8, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 256)               2097408   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 68)                17476     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2207556 (8.42 MB)\n",
            "Trainable params: 2207556 (8.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "129/383 [=========>....................] - ETA: 10s - loss: 4.1385 - accuracy: 0.0233"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-32758879c303>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test M2a with increased dropout rate\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_5.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'categorical' because your model uses categorical crossentropy\n",
        "    color_mode='grayscale',  # Match the color mode to how the model was trained\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5rNfncTU6hd",
        "outputId": "55e4e635-a960-4aa3-f129-e158b443bf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_19 (Conv2D)          (None, 64, 64, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPooli  (None, 32, 32, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPooli  (None, 16, 16, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPooli  (None, 8, 8, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 256)               2097408   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 68)                17476     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2207556 (8.42 MB)\n",
            "Trainable params: 2207556 (8.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 6s 61ms/step - loss: 1.0036 - accuracy: 0.6324\n",
            "Test Loss: 1.0036225318908691\n",
            "Test Accuracy: 0.6323529481887817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#increased drop out with larger batch sizes\n",
        "#this was done just in hopes of faster computation\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64\n",
        "input_shape = (img_height, img_width, 1)  # Assuming grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "num_classes = 68\n",
        "\n",
        "# Build the CNN model with increased dropout rates\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.6),  # Increased dropout rate for higher model regularization\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_5.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuY6TkIKVMHn",
        "outputId": "f83aef01-d3a4-4a72-81db-f151237e0852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6120 images belonging to 68 classes.\n",
            "Found 1496 images belonging to 68 classes.\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_25 (Conv2D)          (None, 64, 64, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPooli  (None, 32, 32, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPooli  (None, 16, 16, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPooli  (None, 8, 8, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 256)               2097408   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 68)                17476     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2207556 (8.42 MB)\n",
            "Trainable params: 2207556 (8.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "96/96 [==============================] - 21s 200ms/step - loss: 4.0711 - accuracy: 0.0327 - val_loss: 3.9070 - val_accuracy: 0.0468\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 19s 203ms/step - loss: 3.7893 - accuracy: 0.0606 - val_loss: 3.3823 - val_accuracy: 0.1116\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 19s 197ms/step - loss: 3.4410 - accuracy: 0.0971 - val_loss: 3.0010 - val_accuracy: 0.1878\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 19s 198ms/step - loss: 3.1719 - accuracy: 0.1294 - val_loss: 2.7389 - val_accuracy: 0.2199\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 19s 200ms/step - loss: 2.9537 - accuracy: 0.1698 - val_loss: 2.5679 - val_accuracy: 0.2640\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 19s 198ms/step - loss: 2.7940 - accuracy: 0.1904 - val_loss: 2.3544 - val_accuracy: 0.3122\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 19s 198ms/step - loss: 2.6681 - accuracy: 0.2186 - val_loss: 2.2655 - val_accuracy: 0.3316\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 18s 191ms/step - loss: 2.5393 - accuracy: 0.2417 - val_loss: 2.1898 - val_accuracy: 0.3282\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 19s 198ms/step - loss: 2.4474 - accuracy: 0.2541 - val_loss: 2.0725 - val_accuracy: 0.3783\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 19s 194ms/step - loss: 2.3558 - accuracy: 0.2654 - val_loss: 1.9258 - val_accuracy: 0.3930\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 19s 193ms/step - loss: 2.2739 - accuracy: 0.2770 - val_loss: 1.8523 - val_accuracy: 0.3991\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 19s 195ms/step - loss: 2.2094 - accuracy: 0.3064 - val_loss: 1.8668 - val_accuracy: 0.3864\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 19s 196ms/step - loss: 2.1629 - accuracy: 0.3149 - val_loss: 1.7519 - val_accuracy: 0.4271\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 19s 201ms/step - loss: 2.1112 - accuracy: 0.3222 - val_loss: 1.7443 - val_accuracy: 0.4118\n",
            "Epoch 15/50\n",
            "96/96 [==============================] - 19s 197ms/step - loss: 2.0826 - accuracy: 0.3216 - val_loss: 1.6849 - val_accuracy: 0.4338\n",
            "Epoch 16/50\n",
            "96/96 [==============================] - 19s 200ms/step - loss: 1.9976 - accuracy: 0.3461 - val_loss: 1.6353 - val_accuracy: 0.4572\n",
            "Epoch 17/50\n",
            "96/96 [==============================] - 19s 197ms/step - loss: 1.9857 - accuracy: 0.3472 - val_loss: 1.6065 - val_accuracy: 0.4706\n",
            "Epoch 18/50\n",
            "96/96 [==============================] - 20s 203ms/step - loss: 1.9591 - accuracy: 0.3557 - val_loss: 1.6502 - val_accuracy: 0.4739\n",
            "Epoch 19/50\n",
            "96/96 [==============================] - 19s 196ms/step - loss: 1.9311 - accuracy: 0.3706 - val_loss: 1.5445 - val_accuracy: 0.4853\n",
            "Epoch 20/50\n",
            "96/96 [==============================] - 19s 195ms/step - loss: 1.9165 - accuracy: 0.3701 - val_loss: 1.5786 - val_accuracy: 0.4646\n",
            "Epoch 21/50\n",
            "96/96 [==============================] - 19s 196ms/step - loss: 1.8963 - accuracy: 0.3770 - val_loss: 1.5886 - val_accuracy: 0.4913\n",
            "Epoch 22/50\n",
            "96/96 [==============================] - 19s 193ms/step - loss: 1.8544 - accuracy: 0.3912 - val_loss: 1.4838 - val_accuracy: 0.5033\n",
            "Epoch 23/50\n",
            "96/96 [==============================] - 20s 204ms/step - loss: 1.8263 - accuracy: 0.4008 - val_loss: 1.5009 - val_accuracy: 0.4973\n",
            "Epoch 24/50\n",
            "96/96 [==============================] - 18s 192ms/step - loss: 1.8066 - accuracy: 0.4018 - val_loss: 1.4516 - val_accuracy: 0.5053\n",
            "Epoch 25/50\n",
            "96/96 [==============================] - 19s 200ms/step - loss: 1.8058 - accuracy: 0.3962 - val_loss: 1.4247 - val_accuracy: 0.5214\n",
            "Epoch 26/50\n",
            "96/96 [==============================] - 19s 200ms/step - loss: 1.7591 - accuracy: 0.4157 - val_loss: 1.4095 - val_accuracy: 0.5127\n",
            "Epoch 27/50\n",
            "96/96 [==============================] - 19s 202ms/step - loss: 1.7511 - accuracy: 0.4132 - val_loss: 1.4324 - val_accuracy: 0.5140\n",
            "Epoch 28/50\n",
            "96/96 [==============================] - 19s 202ms/step - loss: 1.7108 - accuracy: 0.4209 - val_loss: 1.3808 - val_accuracy: 0.5281\n",
            "Epoch 29/50\n",
            "96/96 [==============================] - 19s 198ms/step - loss: 1.6863 - accuracy: 0.4219 - val_loss: 1.4102 - val_accuracy: 0.5154\n",
            "Epoch 30/50\n",
            "96/96 [==============================] - 19s 195ms/step - loss: 1.7036 - accuracy: 0.4268 - val_loss: 1.3986 - val_accuracy: 0.5154\n",
            "Epoch 31/50\n",
            "96/96 [==============================] - 19s 194ms/step - loss: 1.6683 - accuracy: 0.4402 - val_loss: 1.4050 - val_accuracy: 0.5274\n",
            "Epoch 32/50\n",
            "96/96 [==============================] - 19s 195ms/step - loss: 1.6576 - accuracy: 0.4391 - val_loss: 1.3949 - val_accuracy: 0.5147\n",
            "Epoch 33/50\n",
            "96/96 [==============================] - 19s 197ms/step - loss: 1.6802 - accuracy: 0.4359 - val_loss: 1.3344 - val_accuracy: 0.5307\n",
            "Epoch 34/50\n",
            "96/96 [==============================] - 19s 202ms/step - loss: 1.6565 - accuracy: 0.4369 - val_loss: 1.3479 - val_accuracy: 0.5314\n",
            "Epoch 35/50\n",
            "96/96 [==============================] - 19s 203ms/step - loss: 1.6532 - accuracy: 0.4402 - val_loss: 1.3440 - val_accuracy: 0.5307\n",
            "Epoch 36/50\n",
            "96/96 [==============================] - 19s 203ms/step - loss: 1.6319 - accuracy: 0.4515 - val_loss: 1.3028 - val_accuracy: 0.5561\n",
            "Epoch 37/50\n",
            "96/96 [==============================] - 19s 195ms/step - loss: 1.5938 - accuracy: 0.4606 - val_loss: 1.2392 - val_accuracy: 0.5655\n",
            "Epoch 38/50\n",
            "96/96 [==============================] - 19s 193ms/step - loss: 1.5994 - accuracy: 0.4616 - val_loss: 1.2760 - val_accuracy: 0.5521\n",
            "Epoch 39/50\n",
            "96/96 [==============================] - 19s 196ms/step - loss: 1.6007 - accuracy: 0.4534 - val_loss: 1.2642 - val_accuracy: 0.5441\n",
            "Epoch 40/50\n",
            "96/96 [==============================] - 19s 197ms/step - loss: 1.6060 - accuracy: 0.4513 - val_loss: 1.2832 - val_accuracy: 0.5642\n",
            "Epoch 41/50\n",
            "96/96 [==============================] - 19s 198ms/step - loss: 1.5720 - accuracy: 0.4616 - val_loss: 1.2899 - val_accuracy: 0.5421\n",
            "Epoch 42/50\n",
            "96/96 [==============================] - 19s 198ms/step - loss: 1.5628 - accuracy: 0.4647 - val_loss: 1.2775 - val_accuracy: 0.5401\n",
            "Epoch 43/50\n",
            "96/96 [==============================] - 19s 199ms/step - loss: 1.5400 - accuracy: 0.4783 - val_loss: 1.2649 - val_accuracy: 0.5515\n",
            "Epoch 44/50\n",
            "96/96 [==============================] - 19s 200ms/step - loss: 1.5453 - accuracy: 0.4739 - val_loss: 1.3006 - val_accuracy: 0.5354\n",
            "Epoch 45/50\n",
            "96/96 [==============================] - 19s 193ms/step - loss: 1.5530 - accuracy: 0.4748 - val_loss: 1.2788 - val_accuracy: 0.5595\n",
            "Epoch 46/50\n",
            "96/96 [==============================] - 19s 200ms/step - loss: 1.5314 - accuracy: 0.4722 - val_loss: 1.2397 - val_accuracy: 0.5749\n",
            "Epoch 47/50\n",
            "96/96 [==============================] - 18s 191ms/step - loss: 1.5500 - accuracy: 0.4662 - val_loss: 1.2154 - val_accuracy: 0.5568\n",
            "Epoch 48/50\n",
            "96/96 [==============================] - 19s 200ms/step - loss: 1.5231 - accuracy: 0.4757 - val_loss: 1.2704 - val_accuracy: 0.5528\n",
            "Epoch 49/50\n",
            "96/96 [==============================] - 19s 199ms/step - loss: 1.5122 - accuracy: 0.4788 - val_loss: 1.2294 - val_accuracy: 0.5668\n",
            "Epoch 50/50\n",
            "96/96 [==============================] - 19s 195ms/step - loss: 1.5107 - accuracy: 0.4832 - val_loss: 1.2211 - val_accuracy: 0.5695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test increased dropout rate and larger batch size\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_5.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'categorical' because your model uses categorical crossentropy\n",
        "    color_mode='grayscale',  # Match the color mode to how the model was trained\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlBG3SfyY_7K",
        "outputId": "b1135b83-d357-4e38-dbcb-3c756bb367bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_25 (Conv2D)          (None, 64, 64, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPooli  (None, 32, 32, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPooli  (None, 16, 16, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPooli  (None, 8, 8, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 256)               2097408   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 68)                17476     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2207556 (8.42 MB)\n",
            "Trainable params: 2207556 (8.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 6s 60ms/step - loss: 1.0561 - accuracy: 0.5990\n",
            "Test Loss: 1.056062936782837\n",
            "Test Accuracy: 0.5989583134651184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adding residual layers to M2a with optimal dropout rates\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64\n",
        "input_shape = (img_height, img_width, 1)  # Assuming grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "num_classes = 68\n",
        "\n",
        "# Build the CNN model with residual connections\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "ident1 = x  # Save for addition later\n",
        "\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Add()([x, ident1])  # Residual connection\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.6)(x)\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_6_res.h5\")\n",
        "\n",
        "##colab wont let me run this anymore but there's proof that it exists.\n"
      ],
      "metadata": {
        "id": "FLNXqRnsZFS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W95tNNadOxSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_6.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/renamed',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'categorical' because your model uses categorical crossentropy\n",
        "    color_mode='grayscale',  # Match the color mode to how the model was trained\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "78LL85Q3ha0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#M2a increased filters and optimal dropout rate + residual layers\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64\n",
        "input_shape = (img_height, img_width, 1)  # Assuming grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "num_classes = 68\n",
        "\n",
        "# Build the CNN model with increased filters and residual connections\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)  # Increased from 32 to 64\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)  # Increased from 64 to 128\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "ident1 = x  # Save for addition later\n",
        "\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)  # Maintained at 128\n",
        "x = Add()([x, ident1])  # Residual connection\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)  # Increased from 128 to 256\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)  # Increased from 256 to 512\n",
        "x = Dropout(0.6)(x)\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary to see the changes\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_7.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxWEJbl_LGfl",
        "outputId": "dbc538c8-4fc3-4141-f0df-3a91b1168472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6120 images belonging to 68 classes.\n",
            "Found 1496 images belonging to 68 classes.\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 64, 64, 64)           640       ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 32, 32, 64)           0         ['conv2d_8[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 128)          73856     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPoolin  (None, 16, 16, 128)          0         ['conv2d_9[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 128)          147584    ['max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 16, 16, 128)          0         ['conv2d_10[0][0]',           \n",
            "                                                                     'max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooli  (None, 8, 8, 128)            0         ['add_2[0][0]']               \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 8, 8, 256)            295168    ['max_pooling2d_10[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling2d_11 (MaxPooli  (None, 4, 4, 256)            0         ['conv2d_11[0][0]']           \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 4096)                 0         ['max_pooling2d_11[0][0]']    \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 512)                  2097664   ['flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 512)                  0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 68)                   34884     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2649796 (10.11 MB)\n",
            "Trainable params: 2649796 (10.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "48/48 [==============================] - 20s 384ms/step - loss: 4.1007 - accuracy: 0.0276 - val_loss: 3.8991 - val_accuracy: 0.0448\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 3.7495 - accuracy: 0.0634 - val_loss: 3.2483 - val_accuracy: 0.1544\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 3.2534 - accuracy: 0.1227 - val_loss: 2.6800 - val_accuracy: 0.2373\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 2.8684 - accuracy: 0.1827 - val_loss: 2.3610 - val_accuracy: 0.3068\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 2.5856 - accuracy: 0.2283 - val_loss: 2.1452 - val_accuracy: 0.3302\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 2.3581 - accuracy: 0.2711 - val_loss: 1.9222 - val_accuracy: 0.3930\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 20s 416ms/step - loss: 2.2071 - accuracy: 0.3069 - val_loss: 1.8150 - val_accuracy: 0.4231\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 2.0683 - accuracy: 0.3379 - val_loss: 1.6894 - val_accuracy: 0.4432\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 1.9753 - accuracy: 0.3600 - val_loss: 1.6414 - val_accuracy: 0.4579\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.9225 - accuracy: 0.3711 - val_loss: 1.5657 - val_accuracy: 0.4652\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 18s 385ms/step - loss: 1.8288 - accuracy: 0.3931 - val_loss: 1.4720 - val_accuracy: 0.4893\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 1.7461 - accuracy: 0.4101 - val_loss: 1.4201 - val_accuracy: 0.5107\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 1.6649 - accuracy: 0.4281 - val_loss: 1.4074 - val_accuracy: 0.5040\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 1.6852 - accuracy: 0.4337 - val_loss: 1.3670 - val_accuracy: 0.5234\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 1.6241 - accuracy: 0.4467 - val_loss: 1.3296 - val_accuracy: 0.5361\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 1.5814 - accuracy: 0.4632 - val_loss: 1.2210 - val_accuracy: 0.5608\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 1.5297 - accuracy: 0.4709 - val_loss: 1.2767 - val_accuracy: 0.5368\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 19s 387ms/step - loss: 1.5179 - accuracy: 0.4804 - val_loss: 1.2346 - val_accuracy: 0.5582\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 1.4943 - accuracy: 0.4825 - val_loss: 1.2742 - val_accuracy: 0.5548\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 18s 385ms/step - loss: 1.4696 - accuracy: 0.5011 - val_loss: 1.2257 - val_accuracy: 0.5702\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 19s 396ms/step - loss: 1.4229 - accuracy: 0.5111 - val_loss: 1.1573 - val_accuracy: 0.5816\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 1.3822 - accuracy: 0.5230 - val_loss: 1.1453 - val_accuracy: 0.5949\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 1.3663 - accuracy: 0.5168 - val_loss: 1.1476 - val_accuracy: 0.5755\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 19s 386ms/step - loss: 1.3334 - accuracy: 0.5307 - val_loss: 1.1198 - val_accuracy: 0.5976\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.3349 - accuracy: 0.5255 - val_loss: 1.1468 - val_accuracy: 0.5782\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 1.2929 - accuracy: 0.5410 - val_loss: 1.1009 - val_accuracy: 0.6103\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 19s 402ms/step - loss: 1.2956 - accuracy: 0.5529 - val_loss: 1.1249 - val_accuracy: 0.5916\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 1.2837 - accuracy: 0.5552 - val_loss: 1.0509 - val_accuracy: 0.6063\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 18s 385ms/step - loss: 1.2981 - accuracy: 0.5404 - val_loss: 1.0638 - val_accuracy: 0.6103\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.2604 - accuracy: 0.5565 - val_loss: 1.0839 - val_accuracy: 0.6150\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 19s 387ms/step - loss: 1.2508 - accuracy: 0.5660 - val_loss: 1.1256 - val_accuracy: 0.5936\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 19s 387ms/step - loss: 1.2107 - accuracy: 0.5636 - val_loss: 1.0433 - val_accuracy: 0.6364\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 19s 399ms/step - loss: 1.2112 - accuracy: 0.5745 - val_loss: 1.0587 - val_accuracy: 0.6076\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 19s 403ms/step - loss: 1.1966 - accuracy: 0.5757 - val_loss: 1.0193 - val_accuracy: 0.6257\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.1847 - accuracy: 0.5797 - val_loss: 0.9718 - val_accuracy: 0.6310\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.1701 - accuracy: 0.5802 - val_loss: 1.0252 - val_accuracy: 0.6223\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 1.1390 - accuracy: 0.5859 - val_loss: 1.0337 - val_accuracy: 0.6210\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.1661 - accuracy: 0.5824 - val_loss: 1.0909 - val_accuracy: 0.6197\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 1.1411 - accuracy: 0.5943 - val_loss: 0.9934 - val_accuracy: 0.6317\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 19s 387ms/step - loss: 1.1352 - accuracy: 0.5956 - val_loss: 0.9629 - val_accuracy: 0.6377\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 19s 398ms/step - loss: 1.1131 - accuracy: 0.6000 - val_loss: 0.9887 - val_accuracy: 0.6223\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 1.0787 - accuracy: 0.6054 - val_loss: 0.9528 - val_accuracy: 0.6491\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 1.1088 - accuracy: 0.6034 - val_loss: 0.9526 - val_accuracy: 0.6497\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.0810 - accuracy: 0.6092 - val_loss: 0.9512 - val_accuracy: 0.6464\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 1.0986 - accuracy: 0.5977 - val_loss: 0.9818 - val_accuracy: 0.6444\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 18s 377ms/step - loss: 1.0814 - accuracy: 0.6152 - val_loss: 0.9648 - val_accuracy: 0.6384\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 1.0426 - accuracy: 0.6158 - val_loss: 0.9257 - val_accuracy: 0.6504\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 1.0567 - accuracy: 0.6136 - val_loss: 0.9298 - val_accuracy: 0.6404\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 1.0500 - accuracy: 0.6141 - val_loss: 0.9730 - val_accuracy: 0.6390\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 18s 376ms/step - loss: 1.0304 - accuracy: 0.6149 - val_loss: 0.9191 - val_accuracy: 0.6604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls #proof that at some point I did have all of these models, I just can't access them anymore for some ungodly reason"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWpQXV9gsfjz",
        "outputId": "a156fe8c-a106-4cac-be8b-771f102f157f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442final.ipynb\t\t\t\t\t\t    mobilenetv3_symbol_classifier_fold_5.h5\n",
            "data\t\t\t\t\t\t\t    mobilenetv3_symbol_classifier_tuned.h5\n",
            "mobilenetv3_symbol_classifier_2.h5\t\t\t    mobilenetv3_symbol_cv_tuned.h5\n",
            "mobilenetv3_symbol_classifier_3.h5\t\t\t    my_dir\n",
            "mobilenetv3_symbol_classifier_5.h5\t\t\t    renamed\n",
            "mobilenetv3_symbol_classifier_6_res_dropout_every_layer.h5  renamed_data.zip\n",
            "mobilenetv3_symbol_classifier_6_res.h5\t\t\t    test_set\n",
            "mobilenetv3_symbol_classifier_7.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_7.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'categorical' because your model uses categorical crossentropy\n",
        "    color_mode='grayscale',  # Match the color mode to how the model was trained\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUVPjXmxdbox",
        "outputId": "e5807923-b171-4b86-8369-85c3e4b4b556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 64, 64, 64)           640       ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 32, 32, 64)           0         ['conv2d_8[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 128)          73856     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPoolin  (None, 16, 16, 128)          0         ['conv2d_9[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 128)          147584    ['max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 16, 16, 128)          0         ['conv2d_10[0][0]',           \n",
            "                                                                     'max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooli  (None, 8, 8, 128)            0         ['add_2[0][0]']               \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 8, 8, 256)            295168    ['max_pooling2d_10[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling2d_11 (MaxPooli  (None, 4, 4, 256)            0         ['conv2d_11[0][0]']           \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 4096)                 0         ['max_pooling2d_11[0][0]']    \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 512)                  2097664   ['flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 512)                  0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 68)                   34884     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2649796 (10.11 MB)\n",
            "Trainable params: 2649796 (10.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 7s 64ms/step - loss: 0.8292 - accuracy: 0.6823\n",
            "Test Loss: 0.8292065262794495\n",
            "Test Accuracy: 0.6822916865348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#added 5 fold cv\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# Assuming dataset_dir is your directory with images classified in subfolders for each class\n",
        "image_files = []\n",
        "labels = []\n",
        "\n",
        "# Load dataset file paths and labels\n",
        "for class_dir in os.listdir(dataset_dir):\n",
        "    class_dir_path = os.path.join(dataset_dir, class_dir)\n",
        "    if os.path.isdir(class_dir_path):\n",
        "        for img in os.listdir(class_dir_path):\n",
        "            image_files.append(os.path.join(class_dir_path, img))\n",
        "            labels.append(class_dir)  # assuming folder names are class labels\n",
        "\n",
        "image_files = np.array(image_files)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Define the K-Folds\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# This will print out the indices for each fold\n",
        "for train_index, val_index in kf.split(image_files):\n",
        "    train_images, val_images = image_files[train_index], image_files[val_index]\n",
        "    train_labels, val_labels = labels[train_index], labels[val_index]\n",
        "\n",
        "    # Now create a data generator for training\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Flow from directory expects directories for each class with images inside,\n",
        "    # so you need to create a temporary directory structure or use flow_from_dataframe\n",
        "\n",
        "    # Assuming the use of flow_from_dataframe if directories are not to be created\n",
        "    train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': train_images, 'class': train_labels}),\n",
        "        directory=None,  # no directory structure used\n",
        "        x_col='filename',\n",
        "        y_col='class',\n",
        "        target_size=(img_height, img_width),\n",
        "        color_mode='grayscale',\n",
        "        class_mode='categorical',\n",
        "        batch_size=128,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': val_images, 'class': val_labels}),\n",
        "        directory=None,  # no directory structure used\n",
        "        x_col='filename',\n",
        "        y_col='class',\n",
        "        target_size=(img_height, img_width),\n",
        "        color_mode='grayscale',\n",
        "        class_mode='categorical',\n",
        "        batch_size=128,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Build, compile, and fit your model inside the loop\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Train the model with callbacks\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=20,\n",
        "        validation_data=val_generator,\n",
        "        callbacks=[early_stopping, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # Optionally save the model\n",
        "    model.save(f\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_fold_5.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izNZf18eeQtR",
        "outputId": "9670ad3d-701a-4bce-e21b-c0023e5aff75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6092 validated image filenames belonging to 68 classes.\n",
            "Found 1524 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 20s 371ms/step - loss: 1.0807 - accuracy: 0.6037 - val_loss: 0.6776 - val_accuracy: 0.7238 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 17s 364ms/step - loss: 1.0582 - accuracy: 0.6080 - val_loss: 0.8447 - val_accuracy: 0.6693 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 18s 369ms/step - loss: 1.0337 - accuracy: 0.6193 - val_loss: 0.7233 - val_accuracy: 0.7257 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 17s 365ms/step - loss: 1.0168 - accuracy: 0.6300 - val_loss: 0.7805 - val_accuracy: 0.6975 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 18s 366ms/step - loss: 1.0463 - accuracy: 0.6124 - val_loss: 0.8777 - val_accuracy: 0.6654 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 18s 378ms/step - loss: 1.0234 - accuracy: 0.6175 - val_loss: 0.7035 - val_accuracy: 0.7172 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 18s 373ms/step - loss: 1.0234 - accuracy: 0.6221 - val_loss: 0.6914 - val_accuracy: 0.7231 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 0.9848 - accuracy: 0.6295 - val_loss: 0.7861 - val_accuracy: 0.6857 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 17s 363ms/step - loss: 0.9548 - accuracy: 0.6431 - val_loss: 0.7499 - val_accuracy: 0.7106 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 18s 369ms/step - loss: 0.9198 - accuracy: 0.6525 - val_loss: 0.7701 - val_accuracy: 0.7014 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 18s 372ms/step - loss: 0.9054 - accuracy: 0.6581 - val_loss: 0.7391 - val_accuracy: 0.7113 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 18s 377ms/step - loss: 0.9000 - accuracy: 0.6550 - val_loss: 0.7500 - val_accuracy: 0.7021 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 18s 379ms/step - loss: 0.9108 - accuracy: 0.6527 - val_loss: 0.8049 - val_accuracy: 0.6903 - lr: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6093 validated image filenames belonging to 68 classes.\n",
            "Found 1523 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 19s 366ms/step - loss: 1.0629 - accuracy: 0.6202 - val_loss: 0.8416 - val_accuracy: 0.6756 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 18s 372ms/step - loss: 1.0206 - accuracy: 0.6268 - val_loss: 0.7438 - val_accuracy: 0.7006 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 18s 371ms/step - loss: 0.9903 - accuracy: 0.6245 - val_loss: 0.7306 - val_accuracy: 0.7183 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 18s 373ms/step - loss: 1.0152 - accuracy: 0.6315 - val_loss: 0.9246 - val_accuracy: 0.6520 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 17s 364ms/step - loss: 0.9975 - accuracy: 0.6398 - val_loss: 0.7712 - val_accuracy: 0.6914 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 17s 361ms/step - loss: 1.0092 - accuracy: 0.6333 - val_loss: 0.8530 - val_accuracy: 0.6586 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 18s 373ms/step - loss: 0.9821 - accuracy: 0.6363 - val_loss: 0.8945 - val_accuracy: 0.6408 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 18s 375ms/step - loss: 0.9883 - accuracy: 0.6404 - val_loss: 0.8520 - val_accuracy: 0.6559 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 17s 365ms/step - loss: 0.9407 - accuracy: 0.6530 - val_loss: 0.7956 - val_accuracy: 0.6789 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 17s 364ms/step - loss: 0.9009 - accuracy: 0.6652 - val_loss: 0.7931 - val_accuracy: 0.6940 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 17s 361ms/step - loss: 0.9043 - accuracy: 0.6634 - val_loss: 0.8916 - val_accuracy: 0.6481 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 17s 360ms/step - loss: 0.8879 - accuracy: 0.6711 - val_loss: 0.7754 - val_accuracy: 0.6914 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 17s 362ms/step - loss: 0.8817 - accuracy: 0.6714 - val_loss: 0.7893 - val_accuracy: 0.6881 - lr: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6093 validated image filenames belonging to 68 classes.\n",
            "Found 1523 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 20s 374ms/step - loss: 1.0296 - accuracy: 0.6220 - val_loss: 0.8263 - val_accuracy: 0.6868 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 17s 362ms/step - loss: 1.0196 - accuracy: 0.6243 - val_loss: 0.7925 - val_accuracy: 0.6861 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 19s 387ms/step - loss: 0.9960 - accuracy: 0.6373 - val_loss: 0.8034 - val_accuracy: 0.6861 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 18s 366ms/step - loss: 0.9972 - accuracy: 0.6389 - val_loss: 0.7592 - val_accuracy: 0.7006 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 18s 369ms/step - loss: 0.9723 - accuracy: 0.6402 - val_loss: 0.7329 - val_accuracy: 0.7098 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 18s 369ms/step - loss: 0.9839 - accuracy: 0.6263 - val_loss: 0.7716 - val_accuracy: 0.6868 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 18s 370ms/step - loss: 0.9658 - accuracy: 0.6398 - val_loss: 0.8401 - val_accuracy: 0.6875 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 17s 364ms/step - loss: 0.9511 - accuracy: 0.6450 - val_loss: 0.8401 - val_accuracy: 0.6776 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 17s 357ms/step - loss: 0.9592 - accuracy: 0.6368 - val_loss: 0.9266 - val_accuracy: 0.6546 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 18s 378ms/step - loss: 0.9559 - accuracy: 0.6466 - val_loss: 0.8429 - val_accuracy: 0.6829 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 18s 369ms/step - loss: 0.9093 - accuracy: 0.6596 - val_loss: 0.7982 - val_accuracy: 0.6888 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 18s 368ms/step - loss: 0.8884 - accuracy: 0.6690 - val_loss: 0.8142 - val_accuracy: 0.6842 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 17s 363ms/step - loss: 0.8691 - accuracy: 0.6724 - val_loss: 0.8226 - val_accuracy: 0.6802 - lr: 2.0000e-04\n",
            "Epoch 14/20\n",
            "48/48 [==============================] - 18s 385ms/step - loss: 0.8597 - accuracy: 0.6732 - val_loss: 0.8663 - val_accuracy: 0.6717 - lr: 2.0000e-04\n",
            "Epoch 15/20\n",
            "48/48 [==============================] - 17s 363ms/step - loss: 0.8606 - accuracy: 0.6731 - val_loss: 0.8163 - val_accuracy: 0.6881 - lr: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6093 validated image filenames belonging to 68 classes.\n",
            "Found 1523 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 20s 371ms/step - loss: 1.0272 - accuracy: 0.6266 - val_loss: 0.6734 - val_accuracy: 0.7150 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 18s 372ms/step - loss: 0.9876 - accuracy: 0.6343 - val_loss: 0.7200 - val_accuracy: 0.6947 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 18s 379ms/step - loss: 0.9798 - accuracy: 0.6309 - val_loss: 0.7654 - val_accuracy: 0.6947 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 18s 368ms/step - loss: 0.9861 - accuracy: 0.6424 - val_loss: 0.8581 - val_accuracy: 0.6678 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 18s 365ms/step - loss: 0.9600 - accuracy: 0.6493 - val_loss: 0.8552 - val_accuracy: 0.6638 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 18s 368ms/step - loss: 0.9839 - accuracy: 0.6430 - val_loss: 0.8055 - val_accuracy: 0.6842 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 18s 367ms/step - loss: 0.9270 - accuracy: 0.6642 - val_loss: 0.7297 - val_accuracy: 0.7006 - lr: 2.0000e-04\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 17s 360ms/step - loss: 0.9090 - accuracy: 0.6611 - val_loss: 0.6880 - val_accuracy: 0.7091 - lr: 2.0000e-04\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 18s 380ms/step - loss: 0.9027 - accuracy: 0.6608 - val_loss: 0.6875 - val_accuracy: 0.7131 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 18s 372ms/step - loss: 0.8752 - accuracy: 0.6663 - val_loss: 0.7276 - val_accuracy: 0.7032 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 17s 354ms/step - loss: 0.8579 - accuracy: 0.6721 - val_loss: 0.6892 - val_accuracy: 0.7104 - lr: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6093 validated image filenames belonging to 68 classes.\n",
            "Found 1523 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 19s 365ms/step - loss: 1.0420 - accuracy: 0.6176 - val_loss: 0.7611 - val_accuracy: 0.6875 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 18s 379ms/step - loss: 1.0128 - accuracy: 0.6309 - val_loss: 0.6718 - val_accuracy: 0.7058 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 17s 361ms/step - loss: 0.9854 - accuracy: 0.6425 - val_loss: 0.7679 - val_accuracy: 0.6947 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 18s 369ms/step - loss: 0.9862 - accuracy: 0.6312 - val_loss: 0.6658 - val_accuracy: 0.7229 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 18s 370ms/step - loss: 0.9919 - accuracy: 0.6402 - val_loss: 0.6956 - val_accuracy: 0.7111 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 18s 368ms/step - loss: 0.9526 - accuracy: 0.6391 - val_loss: 0.7396 - val_accuracy: 0.6986 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 17s 365ms/step - loss: 0.9420 - accuracy: 0.6555 - val_loss: 0.7476 - val_accuracy: 0.6986 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 18s 376ms/step - loss: 0.9424 - accuracy: 0.6471 - val_loss: 0.6965 - val_accuracy: 0.7209 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 18s 370ms/step - loss: 0.9535 - accuracy: 0.6470 - val_loss: 0.7978 - val_accuracy: 0.6750 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 18s 369ms/step - loss: 0.8836 - accuracy: 0.6652 - val_loss: 0.7636 - val_accuracy: 0.6980 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 18s 368ms/step - loss: 0.8790 - accuracy: 0.6701 - val_loss: 0.7635 - val_accuracy: 0.6953 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 18s 375ms/step - loss: 0.8682 - accuracy: 0.6729 - val_loss: 0.7214 - val_accuracy: 0.7104 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 18s 369ms/step - loss: 0.8582 - accuracy: 0.6741 - val_loss: 0.7066 - val_accuracy: 0.7183 - lr: 2.0000e-04\n",
            "Epoch 14/20\n",
            "48/48 [==============================] - 18s 372ms/step - loss: 0.8592 - accuracy: 0.6729 - val_loss: 0.6991 - val_accuracy: 0.7229 - lr: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_fold_5.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'categorical' because your model uses categorical crossentropy\n",
        "    color_mode='grayscale',  # Match the color mode to how the model was trained\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "##this is my best model so far\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evTmsnpRele-",
        "outputId": "73f07364-3013-49fe-84c1-b356b754694e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 64, 64, 64)           640       ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 32, 32, 64)           0         ['conv2d_8[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 128)          73856     ['max_pooling2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPoolin  (None, 16, 16, 128)          0         ['conv2d_9[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 128)          147584    ['max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 16, 16, 128)          0         ['conv2d_10[0][0]',           \n",
            "                                                                     'max_pooling2d_9[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooli  (None, 8, 8, 128)            0         ['add_2[0][0]']               \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 8, 8, 256)            295168    ['max_pooling2d_10[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling2d_11 (MaxPooli  (None, 4, 4, 256)            0         ['conv2d_11[0][0]']           \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 4096)                 0         ['max_pooling2d_11[0][0]']    \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 512)                  2097664   ['flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 512)                  0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 68)                   34884     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2649796 (10.11 MB)\n",
            "Trainable params: 2649796 (10.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 1160s 11s/step - loss: 0.7487 - accuracy: 0.7019\n",
            "Test Loss: 0.7487124800682068\n",
            "Test Accuracy: 0.701899528503418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add dropout layer after every convolution layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define the root directory of your dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/442final/renamed\"\n",
        "\n",
        "# Define the image size and input shape\n",
        "img_height, img_width = 64, 64\n",
        "input_shape = (img_height, img_width, 1)  # Assuming grayscale images\n",
        "\n",
        "# Prepare data with augmented data generation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    color_mode='grayscale')\n",
        "\n",
        "num_classes = 68\n",
        "\n",
        "# CNN model with dropout layers after each convolutional layer\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "x = Dropout(0.3)(x)  # Adding dropout\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Dropout(0.3)(x)  # Adding dropout\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "ident1 = x  # Save for addition later\n",
        "\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Dropout(0.3)(x)  # Adding dropout\n",
        "x = Add()([x, ident1])  # Residual connection\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Dropout(0.4)(x)  # Increased dropout for deeper layers\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Early Stopping and Reduce LR On Plateau Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_6_res_dropout_every_layer.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZpitnIlpPFn",
        "outputId": "3d077749-5fd9-43bd-ed1e-8f19574317fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6120 images belonging to 68 classes.\n",
            "Found 1496 images belonging to 68 classes.\n",
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)          (None, 64, 64, 32)           320       ['input_7[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 64, 64, 32)           0         ['conv2d_26[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_22 (MaxPooli  (None, 32, 32, 32)           0         ['dropout_10[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)          (None, 32, 32, 64)           18496     ['max_pooling2d_22[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 32, 32, 64)           0         ['conv2d_27[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_23 (MaxPooli  (None, 16, 16, 64)           0         ['dropout_11[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)          (None, 16, 16, 64)           36928     ['max_pooling2d_23[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 16, 16, 64)           0         ['conv2d_28[0][0]']           \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 16, 16, 64)           0         ['dropout_12[0][0]',          \n",
            "                                                                     'max_pooling2d_23[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling2d_24 (MaxPooli  (None, 8, 8, 64)             0         ['add_6[0][0]']               \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)          (None, 8, 8, 128)            73856     ['max_pooling2d_24[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 8, 8, 128)            0         ['conv2d_29[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_25 (MaxPooli  (None, 4, 4, 128)            0         ['dropout_13[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)         (None, 2048)                 0         ['max_pooling2d_25[0][0]']    \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 256)                  524544    ['flatten_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 256)                  0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 68)                   17476     ['dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 671620 (2.56 MB)\n",
            "Trainable params: 671620 (2.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "48/48 [==============================] - 21s 395ms/step - loss: 4.0936 - accuracy: 0.0294 - val_loss: 4.0888 - val_accuracy: 0.0468 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 19s 397ms/step - loss: 3.8679 - accuracy: 0.0552 - val_loss: 3.9121 - val_accuracy: 0.0929 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 19s 401ms/step - loss: 3.5686 - accuracy: 0.0827 - val_loss: 3.6620 - val_accuracy: 0.1324 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 19s 402ms/step - loss: 3.2636 - accuracy: 0.1139 - val_loss: 3.4072 - val_accuracy: 0.2380 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 3.0112 - accuracy: 0.1508 - val_loss: 3.3031 - val_accuracy: 0.2386 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 2.7941 - accuracy: 0.1933 - val_loss: 3.0478 - val_accuracy: 0.3235 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 18s 385ms/step - loss: 2.6390 - accuracy: 0.2212 - val_loss: 2.9243 - val_accuracy: 0.3422 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 2.4696 - accuracy: 0.2542 - val_loss: 2.8024 - val_accuracy: 0.3824 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 18s 379ms/step - loss: 2.3360 - accuracy: 0.2727 - val_loss: 2.6947 - val_accuracy: 0.4017 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 2.2277 - accuracy: 0.3038 - val_loss: 2.5924 - val_accuracy: 0.3884 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 2.1776 - accuracy: 0.3193 - val_loss: 2.5541 - val_accuracy: 0.4412 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 2.0560 - accuracy: 0.3453 - val_loss: 2.3804 - val_accuracy: 0.4459 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 18s 379ms/step - loss: 1.9971 - accuracy: 0.3505 - val_loss: 2.3822 - val_accuracy: 0.4626 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 1.9404 - accuracy: 0.3647 - val_loss: 2.3546 - val_accuracy: 0.4766 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 1.8889 - accuracy: 0.3783 - val_loss: 2.2588 - val_accuracy: 0.4686 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 19s 397ms/step - loss: 1.8967 - accuracy: 0.3732 - val_loss: 2.2023 - val_accuracy: 0.4739 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 18s 375ms/step - loss: 1.8607 - accuracy: 0.3920 - val_loss: 2.2619 - val_accuracy: 0.5140 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 18s 378ms/step - loss: 1.7991 - accuracy: 0.4034 - val_loss: 2.1555 - val_accuracy: 0.5114 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 1.7879 - accuracy: 0.3974 - val_loss: 2.0858 - val_accuracy: 0.5040 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 19s 400ms/step - loss: 1.7622 - accuracy: 0.4129 - val_loss: 2.0489 - val_accuracy: 0.5120 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.7343 - accuracy: 0.4188 - val_loss: 2.1296 - val_accuracy: 0.5114 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 19s 397ms/step - loss: 1.7108 - accuracy: 0.4201 - val_loss: 2.0530 - val_accuracy: 0.5341 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 1.6557 - accuracy: 0.4394 - val_loss: 2.0601 - val_accuracy: 0.5060 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 18s 375ms/step - loss: 1.6536 - accuracy: 0.4454 - val_loss: 1.9759 - val_accuracy: 0.5374 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 19s 397ms/step - loss: 1.6448 - accuracy: 0.4469 - val_loss: 1.9034 - val_accuracy: 0.5622 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 1.6071 - accuracy: 0.4585 - val_loss: 1.9855 - val_accuracy: 0.5361 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.5816 - accuracy: 0.4629 - val_loss: 1.8997 - val_accuracy: 0.5628 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.5419 - accuracy: 0.4706 - val_loss: 1.8814 - val_accuracy: 0.5615 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.5714 - accuracy: 0.4686 - val_loss: 1.9116 - val_accuracy: 0.5508 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 19s 399ms/step - loss: 1.5567 - accuracy: 0.4730 - val_loss: 1.9108 - val_accuracy: 0.5648 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.5192 - accuracy: 0.4850 - val_loss: 1.8325 - val_accuracy: 0.5488 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 19s 400ms/step - loss: 1.4950 - accuracy: 0.4797 - val_loss: 1.8999 - val_accuracy: 0.5802 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 1.4695 - accuracy: 0.4895 - val_loss: 1.8110 - val_accuracy: 0.5541 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 1.4757 - accuracy: 0.4874 - val_loss: 1.8047 - val_accuracy: 0.5521 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 1.4613 - accuracy: 0.4866 - val_loss: 1.8517 - val_accuracy: 0.5582 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.4671 - accuracy: 0.4938 - val_loss: 1.7854 - val_accuracy: 0.5501 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 19s 399ms/step - loss: 1.4316 - accuracy: 0.5016 - val_loss: 1.7835 - val_accuracy: 0.5762 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 19s 399ms/step - loss: 1.4081 - accuracy: 0.5222 - val_loss: 1.7168 - val_accuracy: 0.5949 - lr: 2.0000e-04\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.3446 - accuracy: 0.5242 - val_loss: 1.6967 - val_accuracy: 0.5963 - lr: 2.0000e-04\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.3271 - accuracy: 0.5353 - val_loss: 1.6626 - val_accuracy: 0.5929 - lr: 2.0000e-04\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 1.3367 - accuracy: 0.5369 - val_loss: 1.6646 - val_accuracy: 0.5849 - lr: 2.0000e-04\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 1.3300 - accuracy: 0.5232 - val_loss: 1.6630 - val_accuracy: 0.5989 - lr: 2.0000e-04\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 1.3137 - accuracy: 0.5325 - val_loss: 1.6410 - val_accuracy: 0.6049 - lr: 2.0000e-04\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 1.2868 - accuracy: 0.5475 - val_loss: 1.6170 - val_accuracy: 0.5909 - lr: 2.0000e-04\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 1.3246 - accuracy: 0.5324 - val_loss: 1.6834 - val_accuracy: 0.5989 - lr: 2.0000e-04\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.3133 - accuracy: 0.5350 - val_loss: 1.6447 - val_accuracy: 0.5983 - lr: 2.0000e-04\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 18s 385ms/step - loss: 1.3279 - accuracy: 0.5359 - val_loss: 1.6664 - val_accuracy: 0.5976 - lr: 2.0000e-04\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 18s 380ms/step - loss: 1.3039 - accuracy: 0.5418 - val_loss: 1.6480 - val_accuracy: 0.5836 - lr: 2.0000e-04\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 18s 378ms/step - loss: 1.2841 - accuracy: 0.5490 - val_loss: 1.6366 - val_accuracy: 0.5996 - lr: 1.0000e-04\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.2884 - accuracy: 0.5425 - val_loss: 1.6302 - val_accuracy: 0.5949 - lr: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_6_res_dropout_every_layer.h5\")\n",
        "model.summary()\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale',\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGXh8cspqSng",
        "outputId": "14211b06-49af-49af-8c4c-0f0c37409729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)          (None, 64, 64, 32)           320       ['input_7[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 64, 64, 32)           0         ['conv2d_26[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_22 (MaxPooli  (None, 32, 32, 32)           0         ['dropout_10[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)          (None, 32, 32, 64)           18496     ['max_pooling2d_22[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 32, 32, 64)           0         ['conv2d_27[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_23 (MaxPooli  (None, 16, 16, 64)           0         ['dropout_11[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)          (None, 16, 16, 64)           36928     ['max_pooling2d_23[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 16, 16, 64)           0         ['conv2d_28[0][0]']           \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 16, 16, 64)           0         ['dropout_12[0][0]',          \n",
            "                                                                     'max_pooling2d_23[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling2d_24 (MaxPooli  (None, 8, 8, 64)             0         ['add_6[0][0]']               \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)          (None, 8, 8, 128)            73856     ['max_pooling2d_24[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 8, 8, 128)            0         ['conv2d_29[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_25 (MaxPooli  (None, 4, 4, 128)            0         ['dropout_13[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)         (None, 2048)                 0         ['max_pooling2d_25[0][0]']    \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 256)                  524544    ['flatten_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 256)                  0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 68)                   17476     ['dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 671620 (2.56 MB)\n",
            "Trainable params: 671620 (2.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 7s 61ms/step - loss: 1.3491 - accuracy: 0.6495\n",
            "Test Loss: 1.349136471748352\n",
            "Test Accuracy: 0.6495097875595093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUgK8EBBC_fe",
        "outputId": "c5bf87a3-02e0-49b7-845f-d3de94fd8e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the more filters M2a with keras tuning for hyper params\n",
        "import kerastuner as kt\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def build_model(hp):\n",
        "    # Hyperparameters\n",
        "    num_filters = hp.Int('num_filters', min_value=32, max_value=256, step=32)\n",
        "    kernel_size = hp.Choice('kernel_size', values=[3, 5])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.6, step=0.1)\n",
        "\n",
        "    inputs = Input(shape=(64, 64, 1))\n",
        "    x = Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    ident1 = x\n",
        "    x = Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu', padding='same')(x)\n",
        "    x = Add()([x, ident1])\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(num_filters * 4, (kernel_size, kernel_size), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(68, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Prepare data\n",
        "datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
        "                             shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest', validation_split=0.2)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(dataset_dir, target_size=(64, 64), batch_size=128,\n",
        "                                              class_mode='categorical', subset='training', color_mode='grayscale')\n",
        "val_generator = datagen.flow_from_directory(dataset_dir, target_size=(64, 64), batch_size=128,\n",
        "                                            class_mode='categorical', subset='validation', color_mode='grayscale')\n",
        "\n",
        "# Setup the tuner\n",
        "tuner = kt.Hyperband(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "tuner.search(train_generator, epochs=50, validation_data=val_generator, callbacks=[EarlyStopping(monitor='val_accuracy', patience=5)])\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(train_generator, epochs=50, validation_data=val_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbu62eqrFHWW",
        "outputId": "a6c7dff3-a13b-483b-a059-8f594fe878f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 03m 31s]\n",
            "val_accuracy: 0.48262032866477966\n",
            "\n",
            "Best val_accuracy So Far: 0.5387700796127319\n",
            "Total elapsed time: 00h 42m 02s\n",
            "Epoch 1/50\n",
            "48/48 [==============================] - 20s 388ms/step - loss: 3.9901 - accuracy: 0.0392 - val_loss: 3.6233 - val_accuracy: 0.0715\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 19s 398ms/step - loss: 3.2719 - accuracy: 0.1199 - val_loss: 2.6718 - val_accuracy: 0.2106\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 2.6456 - accuracy: 0.2297 - val_loss: 2.2141 - val_accuracy: 0.3048\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 18s 380ms/step - loss: 2.2151 - accuracy: 0.3142 - val_loss: 1.8504 - val_accuracy: 0.3864\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.9639 - accuracy: 0.3680 - val_loss: 1.6923 - val_accuracy: 0.4465\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 20s 420ms/step - loss: 1.7825 - accuracy: 0.4167 - val_loss: 1.5874 - val_accuracy: 0.4672\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 1.6583 - accuracy: 0.4474 - val_loss: 1.5666 - val_accuracy: 0.4606\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 1.5616 - accuracy: 0.4611 - val_loss: 1.4188 - val_accuracy: 0.5033\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 1.4876 - accuracy: 0.4861 - val_loss: 1.4017 - val_accuracy: 0.5368\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.4245 - accuracy: 0.5029 - val_loss: 1.2871 - val_accuracy: 0.5388\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 1.3653 - accuracy: 0.5209 - val_loss: 1.2927 - val_accuracy: 0.5388\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 1.3204 - accuracy: 0.5332 - val_loss: 1.2362 - val_accuracy: 0.5541\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 1.2695 - accuracy: 0.5410 - val_loss: 1.2143 - val_accuracy: 0.5535\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 19s 398ms/step - loss: 1.2504 - accuracy: 0.5567 - val_loss: 1.1862 - val_accuracy: 0.5675\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 1.2085 - accuracy: 0.5582 - val_loss: 1.1721 - val_accuracy: 0.5762\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 1.1827 - accuracy: 0.5583 - val_loss: 1.1504 - val_accuracy: 0.5695\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 1.1681 - accuracy: 0.5675 - val_loss: 1.1616 - val_accuracy: 0.5682\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 18s 385ms/step - loss: 1.1310 - accuracy: 0.5840 - val_loss: 1.1071 - val_accuracy: 0.5809\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 1.1255 - accuracy: 0.5845 - val_loss: 1.1187 - val_accuracy: 0.5856\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 1.0925 - accuracy: 0.5873 - val_loss: 1.0859 - val_accuracy: 0.6029\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 1.0548 - accuracy: 0.6096 - val_loss: 1.1048 - val_accuracy: 0.5849\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 1.0665 - accuracy: 0.5990 - val_loss: 1.1021 - val_accuracy: 0.6049\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 19s 400ms/step - loss: 1.0397 - accuracy: 0.6082 - val_loss: 1.0610 - val_accuracy: 0.6003\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 18s 380ms/step - loss: 1.0297 - accuracy: 0.6119 - val_loss: 1.0863 - val_accuracy: 0.5936\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 1.0216 - accuracy: 0.6114 - val_loss: 1.0448 - val_accuracy: 0.6190\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 19s 400ms/step - loss: 0.9900 - accuracy: 0.6268 - val_loss: 1.0558 - val_accuracy: 0.6217\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 19s 386ms/step - loss: 0.9739 - accuracy: 0.6255 - val_loss: 1.0001 - val_accuracy: 0.6330\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 18s 379ms/step - loss: 0.9642 - accuracy: 0.6317 - val_loss: 1.0412 - val_accuracy: 0.6103\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 0.9524 - accuracy: 0.6387 - val_loss: 0.9899 - val_accuracy: 0.6210\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 19s 398ms/step - loss: 0.9474 - accuracy: 0.6315 - val_loss: 0.9725 - val_accuracy: 0.6310\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 18s 373ms/step - loss: 0.8985 - accuracy: 0.6551 - val_loss: 1.0219 - val_accuracy: 0.6223\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 0.9128 - accuracy: 0.6467 - val_loss: 0.9899 - val_accuracy: 0.6404\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 0.8818 - accuracy: 0.6582 - val_loss: 1.0061 - val_accuracy: 0.6337\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 18s 377ms/step - loss: 0.8718 - accuracy: 0.6583 - val_loss: 0.9147 - val_accuracy: 0.6624\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.8689 - accuracy: 0.6675 - val_loss: 0.9538 - val_accuracy: 0.6364\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.8716 - accuracy: 0.6621 - val_loss: 0.9970 - val_accuracy: 0.6163\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 0.8553 - accuracy: 0.6709 - val_loss: 0.9821 - val_accuracy: 0.6317\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 0.8614 - accuracy: 0.6667 - val_loss: 0.9601 - val_accuracy: 0.6337\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 0.8306 - accuracy: 0.6781 - val_loss: 0.9324 - val_accuracy: 0.6504\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 19s 391ms/step - loss: 0.8166 - accuracy: 0.6827 - val_loss: 0.9431 - val_accuracy: 0.6303\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 18s 377ms/step - loss: 0.8306 - accuracy: 0.6824 - val_loss: 0.9836 - val_accuracy: 0.6424\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 18s 380ms/step - loss: 0.7936 - accuracy: 0.6912 - val_loss: 0.9349 - val_accuracy: 0.6491\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 0.8038 - accuracy: 0.6900 - val_loss: 0.9059 - val_accuracy: 0.6551\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 19s 397ms/step - loss: 0.7928 - accuracy: 0.6850 - val_loss: 0.9225 - val_accuracy: 0.6618\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 18s 387ms/step - loss: 0.7730 - accuracy: 0.6946 - val_loss: 0.9284 - val_accuracy: 0.6451\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 18s 371ms/step - loss: 0.7607 - accuracy: 0.7010 - val_loss: 0.9652 - val_accuracy: 0.6631\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 0.7597 - accuracy: 0.6992 - val_loss: 0.9174 - val_accuracy: 0.6631\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 18s 379ms/step - loss: 0.7676 - accuracy: 0.7031 - val_loss: 0.9165 - val_accuracy: 0.6644\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 0.7376 - accuracy: 0.7020 - val_loss: 0.9263 - val_accuracy: 0.6591\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 18s 378ms/step - loss: 0.7353 - accuracy: 0.7034 - val_loss: 0.9316 - val_accuracy: 0.6517\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d6cb8d09300>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_tuned.h5\")\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FjBkEzBFMHU",
        "outputId": "4af1638c-7477-4376-e56a-7f616339ce49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 64)           1664      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 32, 32, 64)           0         ['conv2d_4[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 128)          204928    ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPoolin  (None, 16, 16, 128)          0         ['conv2d_5[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 128)          409728    ['max_pooling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 16, 16, 128)          0         ['conv2d_6[0][0]',            \n",
            "                                                                     'max_pooling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPoolin  (None, 8, 8, 128)            0         ['add_1[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 256)            819456    ['max_pooling2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 4, 4, 256)            0         ['conv2d_7[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 4096)                 0         ['max_pooling2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 512)                  2097664   ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 512)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 68)                   34884     ['dropout_1[0][0]']           \n",
            "                                                                                                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================================================================\n",
            "Total params: 3568324 (13.61 MB)\n",
            "Trainable params: 3568324 (13.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_tuned.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'categorical' because your model uses categorical crossentropy\n",
        "    color_mode='grayscale',  # Match the color mode to how the model was trained\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDH67W9oS8dF",
        "outputId": "105231e7-145a-40d7-ff53-9f926a39aa55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 64)           1664      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 32, 32, 64)           0         ['conv2d_4[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 128)          204928    ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPoolin  (None, 16, 16, 128)          0         ['conv2d_5[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 128)          409728    ['max_pooling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 16, 16, 128)          0         ['conv2d_6[0][0]',            \n",
            "                                                                     'max_pooling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPoolin  (None, 8, 8, 128)            0         ['add_1[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 256)            819456    ['max_pooling2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 4, 4, 256)            0         ['conv2d_7[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 4096)                 0         ['max_pooling2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 512)                  2097664   ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 512)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 68)                   34884     ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3568324 (13.61 MB)\n",
            "Trainable params: 3568324 (13.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 7s 62ms/step - loss: 0.7496 - accuracy: 0.7028\n",
            "Test Loss: 0.7495986223220825\n",
            "Test Accuracy: 0.7028186321258545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keras tuned model as base k-fold c.v\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/442final/mobilenetv3_symbol_classifier_tuned.h5')\n",
        "\n",
        "# Set up K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "results = []\n",
        "\n",
        "# Assuming dataset_dir is your directory with images classified in subfolders for each class\n",
        "image_files = []\n",
        "labels = []\n",
        "\n",
        "# Load dataset file paths and labels\n",
        "for class_dir in os.listdir(dataset_dir):\n",
        "    class_dir_path = os.path.join(dataset_dir, class_dir)\n",
        "    if os.path.isdir(class_dir_path):\n",
        "        for img in os.listdir(class_dir_path):\n",
        "            image_files.append(os.path.join(class_dir_path, img))\n",
        "            labels.append(class_dir)  # assuming folder names are class labels\n",
        "\n",
        "image_files = np.array(image_files)\n",
        "labels = np.array(labels)\n",
        "\n",
        "for train_index, val_index in kf.split(image_files):\n",
        "    train_images, val_images = image_files[train_index], image_files[val_index]\n",
        "    train_labels, val_labels = labels[train_index], labels[val_index]\n",
        "\n",
        "    # Prepare training and validation data using flow_from_dataframe\n",
        "    train_data = pd.DataFrame({\n",
        "        'filename': train_images,\n",
        "        'class': train_labels\n",
        "    })\n",
        "\n",
        "    val_data = pd.DataFrame({\n",
        "        'filename': val_images,\n",
        "        'class': val_labels\n",
        "    })\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=train_data,\n",
        "        x_col='filename',\n",
        "        y_col='class',\n",
        "        target_size=(img_height, img_width),\n",
        "        color_mode='grayscale',\n",
        "        class_mode='categorical',\n",
        "        batch_size=128,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=val_data,\n",
        "        x_col='filename',\n",
        "        y_col='class',\n",
        "        target_size=(img_height, img_width),\n",
        "        color_mode='grayscale',\n",
        "        class_mode='categorical',\n",
        "        batch_size=128,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Define callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Train your model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=20,\n",
        "        validation_data=val_generator,\n",
        "        callbacks=[early_stopping, reduce_lr]\n",
        "    )\n",
        "\n",
        "    results.append(history.history)\n",
        "\n",
        "# Output or examine your results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAtNQI7nTNI-",
        "outputId": "6e191ac0-1e25-4bad-cdd1-091c4798565c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6092 validated image filenames belonging to 68 classes.\n",
            "Found 1524 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 21s 389ms/step - loss: 0.8842 - accuracy: 0.6564 - val_loss: 0.7796 - val_accuracy: 0.6975 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 0.8405 - accuracy: 0.6793 - val_loss: 0.8056 - val_accuracy: 0.6870 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 19s 391ms/step - loss: 0.8240 - accuracy: 0.6811 - val_loss: 0.8194 - val_accuracy: 0.6929 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 0.8055 - accuracy: 0.6935 - val_loss: 0.8207 - val_accuracy: 0.6870 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 0.8137 - accuracy: 0.6861 - val_loss: 0.7977 - val_accuracy: 0.6949 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 0.8114 - accuracy: 0.6850 - val_loss: 0.8277 - val_accuracy: 0.6850 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 19s 391ms/step - loss: 0.7249 - accuracy: 0.7106 - val_loss: 0.7505 - val_accuracy: 0.7119 - lr: 2.0000e-04\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 0.6773 - accuracy: 0.7280 - val_loss: 0.7091 - val_accuracy: 0.7192 - lr: 2.0000e-04\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 0.6859 - accuracy: 0.7218 - val_loss: 0.7312 - val_accuracy: 0.7146 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 19s 406ms/step - loss: 0.6614 - accuracy: 0.7370 - val_loss: 0.6988 - val_accuracy: 0.7244 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 0.6509 - accuracy: 0.7349 - val_loss: 0.6971 - val_accuracy: 0.7290 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 19s 407ms/step - loss: 0.6480 - accuracy: 0.7379 - val_loss: 0.7141 - val_accuracy: 0.7133 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 0.6338 - accuracy: 0.7402 - val_loss: 0.6871 - val_accuracy: 0.7198 - lr: 2.0000e-04\n",
            "Epoch 14/20\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.6412 - accuracy: 0.7423 - val_loss: 0.7253 - val_accuracy: 0.7106 - lr: 2.0000e-04\n",
            "Epoch 15/20\n",
            "48/48 [==============================] - 18s 378ms/step - loss: 0.6316 - accuracy: 0.7446 - val_loss: 0.6940 - val_accuracy: 0.7159 - lr: 2.0000e-04\n",
            "Epoch 16/20\n",
            "48/48 [==============================] - 18s 385ms/step - loss: 0.6111 - accuracy: 0.7518 - val_loss: 0.6912 - val_accuracy: 0.7198 - lr: 2.0000e-04\n",
            "Epoch 17/20\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 0.6153 - accuracy: 0.7505 - val_loss: 0.7059 - val_accuracy: 0.7270 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 0.6044 - accuracy: 0.7592 - val_loss: 0.6988 - val_accuracy: 0.7329 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "48/48 [==============================] - 18s 368ms/step - loss: 0.6056 - accuracy: 0.7482 - val_loss: 0.6695 - val_accuracy: 0.7211 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 0.6065 - accuracy: 0.7515 - val_loss: 0.6681 - val_accuracy: 0.7329 - lr: 1.0000e-04\n",
            "Found 6093 validated image filenames belonging to 68 classes.\n",
            "Found 1523 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 20s 390ms/step - loss: 0.8122 - accuracy: 0.6829 - val_loss: 0.7252 - val_accuracy: 0.7196 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 18s 371ms/step - loss: 0.7577 - accuracy: 0.7072 - val_loss: 0.7362 - val_accuracy: 0.7144 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 0.7481 - accuracy: 0.7046 - val_loss: 0.7433 - val_accuracy: 0.7052 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 0.7568 - accuracy: 0.7069 - val_loss: 0.7469 - val_accuracy: 0.7032 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 0.7289 - accuracy: 0.7049 - val_loss: 0.7809 - val_accuracy: 0.6973 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 18s 377ms/step - loss: 0.7240 - accuracy: 0.7082 - val_loss: 0.7679 - val_accuracy: 0.6881 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 18s 374ms/step - loss: 0.6613 - accuracy: 0.7358 - val_loss: 0.6984 - val_accuracy: 0.7203 - lr: 2.0000e-04\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 18s 376ms/step - loss: 0.6170 - accuracy: 0.7468 - val_loss: 0.6754 - val_accuracy: 0.7269 - lr: 2.0000e-04\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 18s 376ms/step - loss: 0.6261 - accuracy: 0.7477 - val_loss: 0.6933 - val_accuracy: 0.7131 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 0.5931 - accuracy: 0.7592 - val_loss: 0.6389 - val_accuracy: 0.7413 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 19s 385ms/step - loss: 0.5849 - accuracy: 0.7527 - val_loss: 0.6882 - val_accuracy: 0.7295 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.5855 - accuracy: 0.7635 - val_loss: 0.6557 - val_accuracy: 0.7426 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 0.5863 - accuracy: 0.7650 - val_loss: 0.6527 - val_accuracy: 0.7275 - lr: 2.0000e-04\n",
            "Epoch 14/20\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 0.5833 - accuracy: 0.7591 - val_loss: 0.6495 - val_accuracy: 0.7249 - lr: 2.0000e-04\n",
            "Epoch 15/20\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 0.5598 - accuracy: 0.7660 - val_loss: 0.6752 - val_accuracy: 0.7177 - lr: 2.0000e-04\n",
            "Epoch 16/20\n",
            "48/48 [==============================] - 18s 379ms/step - loss: 0.5781 - accuracy: 0.7642 - val_loss: 0.6845 - val_accuracy: 0.7118 - lr: 2.0000e-04\n",
            "Epoch 17/20\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 0.5686 - accuracy: 0.7655 - val_loss: 0.6619 - val_accuracy: 0.7262 - lr: 2.0000e-04\n",
            "Epoch 18/20\n",
            "48/48 [==============================] - 18s 376ms/step - loss: 0.5537 - accuracy: 0.7760 - val_loss: 0.6941 - val_accuracy: 0.7255 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 0.5587 - accuracy: 0.7671 - val_loss: 0.6816 - val_accuracy: 0.7328 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 0.5530 - accuracy: 0.7707 - val_loss: 0.6529 - val_accuracy: 0.7249 - lr: 1.0000e-04\n",
            "Found 6093 validated image filenames belonging to 68 classes.\n",
            "Found 1523 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 21s 396ms/step - loss: 0.7168 - accuracy: 0.7105 - val_loss: 0.6916 - val_accuracy: 0.7242 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 19s 398ms/step - loss: 0.6952 - accuracy: 0.7230 - val_loss: 0.7283 - val_accuracy: 0.7216 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 18s 375ms/step - loss: 0.6907 - accuracy: 0.7262 - val_loss: 0.7273 - val_accuracy: 0.7157 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 0.6870 - accuracy: 0.7194 - val_loss: 0.6839 - val_accuracy: 0.7308 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 19s 396ms/step - loss: 0.6880 - accuracy: 0.7223 - val_loss: 0.7457 - val_accuracy: 0.7052 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 0.6670 - accuracy: 0.7277 - val_loss: 0.7427 - val_accuracy: 0.7058 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 0.6665 - accuracy: 0.7371 - val_loss: 0.7392 - val_accuracy: 0.7137 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 19s 391ms/step - loss: 0.6592 - accuracy: 0.7399 - val_loss: 0.7490 - val_accuracy: 0.6999 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 0.6602 - accuracy: 0.7254 - val_loss: 0.7690 - val_accuracy: 0.7032 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 0.6045 - accuracy: 0.7515 - val_loss: 0.6889 - val_accuracy: 0.7459 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 18s 380ms/step - loss: 0.5621 - accuracy: 0.7679 - val_loss: 0.6643 - val_accuracy: 0.7301 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 0.5552 - accuracy: 0.7681 - val_loss: 0.6783 - val_accuracy: 0.7439 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 0.5483 - accuracy: 0.7684 - val_loss: 0.6920 - val_accuracy: 0.7321 - lr: 2.0000e-04\n",
            "Epoch 14/20\n",
            "48/48 [==============================] - 19s 396ms/step - loss: 0.5584 - accuracy: 0.7691 - val_loss: 0.6723 - val_accuracy: 0.7466 - lr: 2.0000e-04\n",
            "Epoch 15/20\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.5380 - accuracy: 0.7722 - val_loss: 0.6314 - val_accuracy: 0.7387 - lr: 2.0000e-04\n",
            "Epoch 16/20\n",
            "48/48 [==============================] - 19s 402ms/step - loss: 0.5206 - accuracy: 0.7860 - val_loss: 0.6813 - val_accuracy: 0.7341 - lr: 2.0000e-04\n",
            "Epoch 17/20\n",
            "48/48 [==============================] - 19s 401ms/step - loss: 0.5334 - accuracy: 0.7802 - val_loss: 0.7078 - val_accuracy: 0.7374 - lr: 2.0000e-04\n",
            "Epoch 18/20\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 0.5285 - accuracy: 0.7829 - val_loss: 0.6882 - val_accuracy: 0.7288 - lr: 2.0000e-04\n",
            "Epoch 19/20\n",
            "48/48 [==============================] - 19s 398ms/step - loss: 0.5212 - accuracy: 0.7911 - val_loss: 0.6754 - val_accuracy: 0.7360 - lr: 2.0000e-04\n",
            "Epoch 20/20\n",
            "48/48 [==============================] - 19s 397ms/step - loss: 0.5036 - accuracy: 0.7909 - val_loss: 0.6402 - val_accuracy: 0.7492 - lr: 1.0000e-04\n",
            "Found 6093 validated image filenames belonging to 68 classes.\n",
            "Found 1523 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 21s 393ms/step - loss: 0.6773 - accuracy: 0.7289 - val_loss: 0.5723 - val_accuracy: 0.7623 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 18s 383ms/step - loss: 0.6589 - accuracy: 0.7372 - val_loss: 0.6015 - val_accuracy: 0.7531 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 0.6639 - accuracy: 0.7305 - val_loss: 0.6395 - val_accuracy: 0.7301 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 0.6584 - accuracy: 0.7387 - val_loss: 0.6663 - val_accuracy: 0.7288 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 19s 396ms/step - loss: 0.6490 - accuracy: 0.7366 - val_loss: 0.6548 - val_accuracy: 0.7387 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 0.6461 - accuracy: 0.7399 - val_loss: 0.6149 - val_accuracy: 0.7439 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 0.5894 - accuracy: 0.7628 - val_loss: 0.6021 - val_accuracy: 0.7571 - lr: 2.0000e-04\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 0.5426 - accuracy: 0.7771 - val_loss: 0.5708 - val_accuracy: 0.7689 - lr: 2.0000e-04\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 0.5491 - accuracy: 0.7753 - val_loss: 0.5572 - val_accuracy: 0.7735 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 18s 381ms/step - loss: 0.5335 - accuracy: 0.7820 - val_loss: 0.5424 - val_accuracy: 0.7643 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 18s 378ms/step - loss: 0.5175 - accuracy: 0.7858 - val_loss: 0.5663 - val_accuracy: 0.7597 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 0.5266 - accuracy: 0.7820 - val_loss: 0.5646 - val_accuracy: 0.7656 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.5160 - accuracy: 0.7909 - val_loss: 0.5572 - val_accuracy: 0.7676 - lr: 2.0000e-04\n",
            "Epoch 14/20\n",
            "48/48 [==============================] - 18s 376ms/step - loss: 0.5102 - accuracy: 0.7888 - val_loss: 0.5430 - val_accuracy: 0.7735 - lr: 2.0000e-04\n",
            "Epoch 15/20\n",
            "48/48 [==============================] - 18s 382ms/step - loss: 0.5003 - accuracy: 0.7971 - val_loss: 0.5546 - val_accuracy: 0.7722 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 0.5053 - accuracy: 0.7953 - val_loss: 0.5457 - val_accuracy: 0.7787 - lr: 1.0000e-04\n",
            "Epoch 17/20\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 0.5041 - accuracy: 0.7934 - val_loss: 0.5523 - val_accuracy: 0.7807 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "48/48 [==============================] - 19s 400ms/step - loss: 0.4914 - accuracy: 0.7935 - val_loss: 0.5302 - val_accuracy: 0.7748 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "48/48 [==============================] - 19s 386ms/step - loss: 0.4934 - accuracy: 0.7952 - val_loss: 0.5760 - val_accuracy: 0.7636 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "48/48 [==============================] - 19s 387ms/step - loss: 0.4903 - accuracy: 0.7971 - val_loss: 0.5679 - val_accuracy: 0.7748 - lr: 1.0000e-04\n",
            "Found 6093 validated image filenames belonging to 68 classes.\n",
            "Found 1523 validated image filenames belonging to 68 classes.\n",
            "Epoch 1/20\n",
            "48/48 [==============================] - 20s 387ms/step - loss: 0.6526 - accuracy: 0.7436 - val_loss: 0.5709 - val_accuracy: 0.7708 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 18s 384ms/step - loss: 0.6175 - accuracy: 0.7489 - val_loss: 0.5693 - val_accuracy: 0.7715 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 19s 392ms/step - loss: 0.6143 - accuracy: 0.7517 - val_loss: 0.5824 - val_accuracy: 0.7511 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 19s 394ms/step - loss: 0.5992 - accuracy: 0.7556 - val_loss: 0.6214 - val_accuracy: 0.7360 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 18s 380ms/step - loss: 0.6108 - accuracy: 0.7550 - val_loss: 0.5687 - val_accuracy: 0.7630 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 0.6008 - accuracy: 0.7514 - val_loss: 0.6067 - val_accuracy: 0.7518 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.6160 - accuracy: 0.7482 - val_loss: 0.5965 - val_accuracy: 0.7420 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 0.5483 - accuracy: 0.7801 - val_loss: 0.5740 - val_accuracy: 0.7551 - lr: 2.0000e-04\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 19s 406ms/step - loss: 0.5147 - accuracy: 0.7870 - val_loss: 0.5594 - val_accuracy: 0.7695 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 0.5082 - accuracy: 0.7889 - val_loss: 0.5533 - val_accuracy: 0.7761 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.5092 - accuracy: 0.7855 - val_loss: 0.5553 - val_accuracy: 0.7715 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 19s 395ms/step - loss: 0.4954 - accuracy: 0.7940 - val_loss: 0.5392 - val_accuracy: 0.7715 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 0.4745 - accuracy: 0.7988 - val_loss: 0.5273 - val_accuracy: 0.7735 - lr: 2.0000e-04\n",
            "Epoch 14/20\n",
            "48/48 [==============================] - 19s 388ms/step - loss: 0.4893 - accuracy: 0.7952 - val_loss: 0.5117 - val_accuracy: 0.7794 - lr: 2.0000e-04\n",
            "Epoch 15/20\n",
            "48/48 [==============================] - 20s 421ms/step - loss: 0.4822 - accuracy: 0.7980 - val_loss: 0.5142 - val_accuracy: 0.7643 - lr: 2.0000e-04\n",
            "Epoch 16/20\n",
            "48/48 [==============================] - 19s 389ms/step - loss: 0.4714 - accuracy: 0.8011 - val_loss: 0.5542 - val_accuracy: 0.7649 - lr: 2.0000e-04\n",
            "Epoch 17/20\n",
            "48/48 [==============================] - 19s 393ms/step - loss: 0.4601 - accuracy: 0.8086 - val_loss: 0.5360 - val_accuracy: 0.7669 - lr: 2.0000e-04\n",
            "Epoch 18/20\n",
            "48/48 [==============================] - 19s 390ms/step - loss: 0.4671 - accuracy: 0.8050 - val_loss: 0.5263 - val_accuracy: 0.7820 - lr: 2.0000e-04\n",
            "Epoch 19/20\n",
            "48/48 [==============================] - 18s 377ms/step - loss: 0.4705 - accuracy: 0.8065 - val_loss: 0.5130 - val_accuracy: 0.7741 - lr: 2.0000e-04\n",
            "Epoch 20/20\n",
            "48/48 [==============================] - 19s 397ms/step - loss: 0.4590 - accuracy: 0.8109 - val_loss: 0.5401 - val_accuracy: 0.7702 - lr: 2.0000e-04\n",
            "[{'loss': [0.8842271566390991, 0.8405341506004333, 0.82398921251297, 0.8054722547531128, 0.813733696937561, 0.8114185929298401, 0.724885106086731, 0.6773462295532227, 0.6858842968940735, 0.6613962650299072, 0.6508545279502869, 0.6480461359024048, 0.6338004469871521, 0.6411579251289368, 0.6315833926200867, 0.6111321449279785, 0.6153121590614319, 0.6043810844421387, 0.6055704951286316, 0.606493353843689], 'accuracy': [0.6564346551895142, 0.679251492023468, 0.6810570955276489, 0.6935325264930725, 0.6861457824707031, 0.6849967241287231, 0.7106040716171265, 0.7280039191246033, 0.7217662334442139, 0.7370321750640869, 0.7348982095718384, 0.737852931022644, 0.7401509881019592, 0.7422849535942078, 0.7445830702781677, 0.7518056631088257, 0.7504924535751343, 0.7591924071311951, 0.7481943368911743, 0.7514773607254028], 'val_loss': [0.7795538902282715, 0.8055539131164551, 0.8194122910499573, 0.820661723613739, 0.7976696491241455, 0.8277236819267273, 0.7505032420158386, 0.7091190814971924, 0.7312237024307251, 0.6987892389297485, 0.697130560874939, 0.714081346988678, 0.6870843768119812, 0.7253161072731018, 0.6940235495567322, 0.6912350058555603, 0.7058525085449219, 0.6987560987472534, 0.6695212721824646, 0.6680687665939331], 'val_accuracy': [0.6975065469741821, 0.6870078444480896, 0.6929134130477905, 0.6870078444480896, 0.6948819160461426, 0.6850393414497375, 0.7119422554969788, 0.7191600799560547, 0.7145669460296631, 0.7244094610214233, 0.7290025949478149, 0.7132545709609985, 0.719816267490387, 0.710629940032959, 0.7158792614936829, 0.719816267490387, 0.7270340919494629, 0.7329396605491638, 0.7211285829544067, 0.7329396605491638], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 1e-04, 1e-04, 1e-04, 1e-04]}, {'loss': [0.8122289180755615, 0.757652997970581, 0.7481147646903992, 0.7567809820175171, 0.7289018630981445, 0.7239633202552795, 0.6613375544548035, 0.6170398592948914, 0.6261095404624939, 0.5930644869804382, 0.5848884582519531, 0.5855167508125305, 0.5863009095191956, 0.583333432674408, 0.5597599148750305, 0.5781113505363464, 0.5685554146766663, 0.5537439584732056, 0.5586737394332886, 0.5529599785804749], 'accuracy': [0.6829147934913635, 0.7072049975395203, 0.7045789957046509, 0.7068767547607422, 0.7049072980880737, 0.7081897258758545, 0.735762357711792, 0.7467585802078247, 0.7477433085441589, 0.7592319250106812, 0.7526670098304749, 0.7634990811347961, 0.7649762034416199, 0.7590677738189697, 0.7659609317779541, 0.7641555666923523, 0.7654685974121094, 0.7759724259376526, 0.7671098113059998, 0.7707204818725586], 'val_loss': [0.725219190120697, 0.7362388372421265, 0.7432700395584106, 0.746928870677948, 0.7808704972267151, 0.7678571343421936, 0.6984370946884155, 0.6753548979759216, 0.6933342814445496, 0.6389175057411194, 0.6881784200668335, 0.6556558012962341, 0.6526799201965332, 0.6494510769844055, 0.6751687526702881, 0.6844871044158936, 0.6619477272033691, 0.6940829157829285, 0.6815820336341858, 0.6529057621955872], 'val_accuracy': [0.7196323275566101, 0.7143794894218445, 0.7051871418952942, 0.7032173275947571, 0.6973079442977905, 0.6881155371665955, 0.7202889323234558, 0.7268549203872681, 0.7130663394927979, 0.7413000464439392, 0.7294812798500061, 0.7426132559776306, 0.727511465549469, 0.724885106086731, 0.717662513256073, 0.7117531299591064, 0.7261983156204224, 0.7255417108535767, 0.7327643036842346, 0.724885106086731], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 1e-04, 1e-04, 1e-04]}, {'loss': [0.7168468236923218, 0.6951627135276794, 0.6907458305358887, 0.6870355010032654, 0.6880026459693909, 0.6669632196426392, 0.6665336489677429, 0.6591757535934448, 0.6602221131324768, 0.6044729351997375, 0.5620505213737488, 0.5551838874816895, 0.5482692122459412, 0.5583927631378174, 0.5380469560623169, 0.520567774772644, 0.5333908200263977, 0.5285012125968933, 0.521236002445221, 0.5035972595214844], 'accuracy': [0.710487425327301, 0.7229607701301575, 0.726243257522583, 0.7193500995635986, 0.7223042845726013, 0.727720320224762, 0.7370753288269043, 0.7398654222488403, 0.7254226207733154, 0.7515181303024292, 0.7679303884506226, 0.768094539642334, 0.7684227824211121, 0.7690792679786682, 0.7721976041793823, 0.7859839200973511, 0.7802396416664124, 0.782865583896637, 0.7910717129707336, 0.790907621383667], 'val_loss': [0.6916476488113403, 0.7282658219337463, 0.7272616028785706, 0.6839081645011902, 0.7456550598144531, 0.7426728010177612, 0.7391685247421265, 0.7489573955535889, 0.7690334916114807, 0.6889088153839111, 0.6642874479293823, 0.6782602667808533, 0.6919803023338318, 0.672270655632019, 0.6313814520835876, 0.6813462972640991, 0.7078160643577576, 0.688181459903717, 0.6753982305526733, 0.6401881575584412], 'val_accuracy': [0.7242285013198853, 0.7216020822525024, 0.7156926989555359, 0.7307944893836975, 0.7051871418952942, 0.7058437466621399, 0.7137229442596436, 0.6999343633651733, 0.7032173275947571, 0.7458962798118591, 0.7301378846168518, 0.743926465511322, 0.7321076989173889, 0.7465528845787048, 0.7386736869812012, 0.7340774536132812, 0.7373604774475098, 0.7288246750831604, 0.7360472679138184, 0.7491792440414429], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 1e-04]}, {'loss': [0.6773340702056885, 0.6589133739471436, 0.6638630628585815, 0.658447265625, 0.6490306854248047, 0.6461426019668579, 0.5894397497177124, 0.5425572395324707, 0.5491087436676025, 0.5335315465927124, 0.5174560546875, 0.5266287326812744, 0.5159860253334045, 0.5102115273475647, 0.500325620174408, 0.5052975416183472, 0.5041431188583374, 0.4913778603076935, 0.49340561032295227, 0.4902653992176056], 'accuracy': [0.7288691997528076, 0.7372394800186157, 0.730510413646698, 0.7387165427207947, 0.7365829348564148, 0.7398654222488403, 0.76284259557724, 0.7771213054656982, 0.7753159403800964, 0.7820449471473694, 0.7858197689056396, 0.7820449471473694, 0.790907621383667, 0.7887740135192871, 0.7971442341804504, 0.7953389286994934, 0.7933694124221802, 0.7935335636138916, 0.795174777507782, 0.7971442341804504], 'val_loss': [0.5723416209220886, 0.6014803051948547, 0.6395379900932312, 0.6663066744804382, 0.6547662019729614, 0.6149452328681946, 0.6021041870117188, 0.5708366632461548, 0.5572379231452942, 0.5424144864082336, 0.5663396716117859, 0.5646316409111023, 0.5571573376655579, 0.5429980158805847, 0.5546159744262695, 0.5456616282463074, 0.552268922328949, 0.53020840883255, 0.5760310292243958, 0.5679080486297607], 'val_accuracy': [0.7623112201690674, 0.7531188726425171, 0.7301378846168518, 0.7288246750831604, 0.7386736869812012, 0.743926465511322, 0.7570584416389465, 0.7688772082328796, 0.7734733819961548, 0.7642810344696045, 0.7596848607063293, 0.7655942440032959, 0.7675639986991882, 0.7734733819961548, 0.7721602320671082, 0.7787262201309204, 0.7806959748268127, 0.7747865915298462, 0.7636244297027588, 0.7747865915298462], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04]}, {'loss': [0.6526177525520325, 0.6175282001495361, 0.614294171333313, 0.5992192029953003, 0.6108309626579285, 0.6007651090621948, 0.6160217523574829, 0.5482876896858215, 0.5147067904472351, 0.5082259774208069, 0.509204089641571, 0.49544039368629456, 0.4745165705680847, 0.4892995357513428, 0.48222076892852783, 0.4714490473270416, 0.4601109027862549, 0.46708932518959045, 0.470544695854187, 0.4589676260948181], 'accuracy': [0.7436402440071106, 0.7488921880722046, 0.7516822814941406, 0.7556211948394775, 0.7549647092819214, 0.7513540387153625, 0.7482357025146484, 0.7800754904747009, 0.7869686484336853, 0.7889381051063538, 0.7854915261268616, 0.7940259575843811, 0.7987855076789856, 0.795174777507782, 0.797964870929718, 0.8010832071304321, 0.8086328506469727, 0.8050221800804138, 0.8064992427825928, 0.8109305500984192], 'val_loss': [0.5708639621734619, 0.5692599415779114, 0.5824218988418579, 0.6213650703430176, 0.5687384009361267, 0.6066520810127258, 0.5965301394462585, 0.5740234851837158, 0.5593653917312622, 0.5533376932144165, 0.5552923083305359, 0.5391587018966675, 0.5272814631462097, 0.5116780996322632, 0.514219343662262, 0.5542066097259521, 0.5360377430915833, 0.526329755783081, 0.5129871368408203, 0.5401076674461365], 'val_accuracy': [0.7708470225334167, 0.7715036273002625, 0.75114905834198, 0.7360472679138184, 0.7629678249359131, 0.7518056631088257, 0.7419566512107849, 0.7550886273384094, 0.7695338129997253, 0.7760998010635376, 0.7715036273002625, 0.7715036273002625, 0.7734733819961548, 0.7793828248977661, 0.7642810344696045, 0.7649376392364502, 0.7669073939323425, 0.7820091843605042, 0.7741299867630005, 0.770190417766571], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/442final/mobilenetv3_symbol_cv_tuned.h5')\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nguk4zeTWZvK",
        "outputId": "d3e7b48e-7031-403c-f7df-47326bf1e983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 64)           1664      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 32, 32, 64)           0         ['conv2d_4[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 128)          204928    ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPoolin  (None, 16, 16, 128)          0         ['conv2d_5[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 128)          409728    ['max_pooling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 16, 16, 128)          0         ['conv2d_6[0][0]',            \n",
            "                                                                     'max_pooling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPoolin  (None, 8, 8, 128)            0         ['add_1[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 256)            819456    ['max_pooling2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 4, 4, 256)            0         ['conv2d_7[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 4096)                 0         ['max_pooling2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 512)                  2097664   ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 512)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 68)                   34884     ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3568324 (13.61 MB)\n",
            "Trainable params: 3568324 (13.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/442final/mobilenetv3_symbol_cv_tuned.h5\")\n",
        "model.summary()\n",
        "\n",
        "# Set up the data generator for the test data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Ensure the generator matches the model input requirements\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/442final/test_set',\n",
        "    target_size=(64, 64),  # Match the input size for which the model is configured\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'categorical' because your model uses categorical crossentropy\n",
        "    color_mode='grayscale',  # Match the color mode to how the model was trained\n",
        "    shuffle=False  # Set to False to keep data in the same order for consistency\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "##barely an improvement, did not mention in report\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlKHmMJpifpA",
        "outputId": "783b8402-5f16-46d3-8121-045e3d9c2077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 64, 64, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 64)           1664      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 32, 32, 64)           0         ['conv2d_4[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 128)          204928    ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPoolin  (None, 16, 16, 128)          0         ['conv2d_5[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 128)          409728    ['max_pooling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 16, 16, 128)          0         ['conv2d_6[0][0]',            \n",
            "                                                                     'max_pooling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPoolin  (None, 8, 8, 128)            0         ['add_1[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 256)            819456    ['max_pooling2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 4, 4, 256)            0         ['conv2d_7[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 4096)                 0         ['max_pooling2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 512)                  2097664   ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 512)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 68)                   34884     ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3568324 (13.61 MB)\n",
            "Trainable params: 3568324 (13.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Found 3264 images belonging to 68 classes.\n",
            "102/102 [==============================] - 7s 63ms/step - loss: 0.8231 - accuracy: 0.7086\n",
            "Test Loss: 0.8230651617050171\n",
            "Test Accuracy: 0.7086396813392639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls #all these models existed at some point I swear I don't know why google drive wont let me have them"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj4M6lViz2l5",
        "outputId": "a8b601d6-adea-4b4f-8662-2926ec1c6fa8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442final.ipynb\t\t\t\t\t\t    mobilenetv3_symbol_classifier_fold_5.h5\n",
            "data\t\t\t\t\t\t\t    mobilenetv3_symbol_classifier_tuned.h5\n",
            "mobilenetv3_symbol_classifier_2.h5\t\t\t    mobilenetv3_symbol_cv_tuned.h5\n",
            "mobilenetv3_symbol_classifier_3.h5\t\t\t    my_dir\n",
            "mobilenetv3_symbol_classifier_5.h5\t\t\t    renamed\n",
            "mobilenetv3_symbol_classifier_6_res_dropout_every_layer.h5  renamed_data.zip\n",
            "mobilenetv3_symbol_classifier_6_res.h5\t\t\t    test_set\n",
            "mobilenetv3_symbol_classifier_7.h5\n"
          ]
        }
      ]
    }
  ]
}